{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DSML_WS_11` - Decision Trees\n",
    "\n",
    "In this workshop we take a deep-dive on tree-based methods (and ensembles thereof) commonly used in a myriad of classification and regression problems.\n",
    "\n",
    "We will cover the following: \n",
    "1. **Decision Trees for classification**: breast cancer example\n",
    "1. **Decision Trees for regression**: peak electricity demand example\n",
    "1. **Ensemble methods**: XGBoost, random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Trees for classification: classifying breast cancer cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "cancer_df = pd.read_csv(\"breast_cancer.csv\", index_col=\"id\")\n",
    "cancer_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To abstract from the relatively high-dimensionality of the breast cancer dataset let us confine our analysis to a two-dimensional feature vector consisting of `area_mean` and `concave points_mean` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cells():\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C3')\n",
    "    plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C0')\n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\",fontsize=16)\n",
    "    plt.ylabel(\"Mean Concave Points\",fontsize=16)\n",
    "    plt.legend(['Malignant','Benign'],fontsize=12)\n",
    "    \n",
    "plot_cells()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our X and Y vectors correspondingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(cancer_df[['area_mean','concave points_mean']])\n",
    "Y = cancer_df['diagnosis'].values\n",
    "\n",
    "# recode Y to 0 and 1\n",
    "Y  = np.where(Y==\"M\", int(1), Y) \n",
    "Y  = np.where(Y==\"B\", int(0), Y) \n",
    "Y = Y.astype('int')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not need to normalize, as Decision Trees do not work based on distances across features!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify and fit a simple `DecisionTreeClassifier`, which is available via `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier(max_depth=2,criterion='gini') # we set gini as our impurity measure\n",
    "tree_classifier.fit(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision estimator has an attribute called tree_  which stores the entire tree structure and allows access to low-level attributes. The binary tree_ attribute is represented as a number of parallel arrays. The i-th element of each array holds information about the node `i`. Node 0 is the tree's root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = tree_classifier.tree_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Recall the basic terminology for decision trees, including nodes, leaves, parent and children nodes, threshold and impurity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among those arrays, we have:\n",
    "\n",
    "   - left_child: id of the left child of the node\n",
    "   - right_child: id of the right child of the node\n",
    "   - feature: feature used for splitting the node\n",
    "   - threshold: threshold value used for splitting the node\n",
    "   - impurity: the impurity at the node\n",
    "   - etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign various tree attributes\n",
    "n_nodes = structure.node_count\n",
    "n_leaves = structure.n_leaves\n",
    "children_left = structure.children_left\n",
    "children_right = structure.children_right\n",
    "feature = structure.feature\n",
    "threshold = structure.threshold\n",
    "impurity = structure.impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num nodes: \\t\",n_nodes)\n",
    "print(\"Num leaves: \\t\",n_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Based on this information, can you draw the basic structure of the tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"left children per node: \", children_left)\n",
    "print(\"right children per node: \", children_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Based on this information, can you add the node IDs to your basic tree sketch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision feature at node: \", feature)\n",
    "print(\"Threshold of feature at node\", threshold)\n",
    "print(\"Impurity at node: \", impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Based on this information, can you derive which feature and threshold was used to split each node?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's retrieve the decision path of a selected sample. \n",
    "\n",
    "The `decision_path` method allows us to retrieve the node indicator functions. A non-zero element of an indicator matrix at the position (i, j) indicates that the sample i goes through the node j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_indicators = tree_classifier.decision_path(X) # results in a 569x7 sparse matrix\n",
    "\n",
    "# let's generate a random sample ID\n",
    "sample_id = np.random.randint(0,len(X))\n",
    "\n",
    "# retrieve decision_path for that sample\n",
    "node_index = node_indicators.indices[node_indicators.indptr[sample_id]: #indptr maps the elements of data and indices to the rows of the sparse matrix\n",
    "                                    node_indicators.indptr[sample_id + 1]]  #indptr maps the elements of data and indices to the rows of the sparse matrix\n",
    "\n",
    "print(\"Decision path for sample \" + str(sample_id), \": \", str(node_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply` method can be used to get the index of the leaf that each sample is predicted as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also retrieve the leaf ids reached by each sample using .apply\n",
    "leaf_ids = tree_classifier.apply(X)\n",
    "\n",
    "leaf_ids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: For our basic decision tree, what combinations of IDs could appear as decision paths? What IDs could be returned by  `apply()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us also consider the features and thresholds that were used to predict a certain sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision path for sample %s: %s' % (str(sample_id), str(node_index)))\n",
    "print('Feature values of sample %s: %s \\n' % (sample_id, X[sample_id]))\n",
    "print('Rules used to predict sample %s: ' % sample_id)\n",
    "for node_id in node_index:\n",
    "    # skip leaf node\n",
    "    if leaf_ids[sample_id] == node_id:\n",
    "        continue\n",
    "    \n",
    "    # for all other nodes, retrieve the feature values\n",
    "    if (X[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\"Decision at node %s: value for feature %s (%s) is %s the threshold of %s\"\n",
    "          % (node_id,\n",
    "             feature[node_id],\n",
    "             X[sample_id, feature[node_id]],\n",
    "             threshold_sign,\n",
    "             threshold[node_id]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the full decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "tree.plot_tree(tree_classifier, class_names=['Malignant','Benign'], feature_names=['area_mean', 'concave points_mean'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the decision tree each node is represented by a box. For each node the following information is provided:\n",
    "- decision feature and threshold\n",
    "- impurity\n",
    "- number of samples\n",
    "- number of samples per class\n",
    "- class (i.e., majority vote)\n",
    "\n",
    "We can, thus, easily relate this back to the tree attributes we computed above. A selection is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num nodes: \\t\",n_nodes)\n",
    "print(\"Num leaves: \\t\",n_leaves)\n",
    "print(\"Feature at node\", feature) # -2 indicates no feature/threshold, i.e. a leaf\n",
    "print(\"Threshold of feature at node\", threshold)\n",
    "print(\"Impurity at node: \", impurity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot decision surfaces\n",
    "\n",
    "As we have seen in the lecture, another intuitive representation of decision trees is the use of decision surfaces. These can be related back directly to the decision tree. For ease of use, a plotting routine has been prepared that combines fitting and plotting into a single routine and allows for easy adjustment of tree depth and the minimum samples per leaf (discussed below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_surface(max_depth,min_samples_leaf=1):\n",
    "    \n",
    "    # specify and fit decision tree classifier\n",
    "    from sklearn.tree import DecisionTreeClassifier, export_graphviz # we also call the garphviz module for later visualization\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth,\n",
    "                                   min_samples_leaf=min_samples_leaf,\n",
    "                                  criterion='gini') # we set entropy as our impurity measure\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    # get tree attributes\n",
    "    attributes = model.tree_\n",
    "    \n",
    "    # define range per feature\n",
    "    x_range = [0,2600] # i.e. mean area\n",
    "    y_range = [0, 0.21] # i.e mean conc. points\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    # plot classification regions\n",
    "    grid=1000\n",
    "    xx,yy = np.meshgrid(np.linspace(x_range[0], x_range[1], grid),\n",
    "                        np.linspace(y_range[0], y_range[1], grid))\n",
    "\n",
    "    zz = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    zz = zz.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy,zz,levels=[-float(\"inf\"),0,float(\"inf\")],alpha=0.2,colors=[\"b\",\"r\"])\n",
    "    plt.contour(cs, colors='k')\n",
    "    \n",
    "    # plot data points\n",
    "    s1 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C3')\n",
    "    s2 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C0')    \n",
    "    plt.xlim([0,2600])\n",
    "    plt.ylim([0,0.21])\n",
    "    plt.xlabel(\"Mean Area\",fontsize=16)\n",
    "    plt.ylabel(\"Mean Concave Points\",fontsize=16)\n",
    "    plt.legend([s1,s2],['Malignant','Benign'],fontsize=12)\n",
    "    \n",
    "    print(\"number of nodes: \", attributes.node_count)\n",
    "    print(\"number of leafs: \", attributes.n_leaves)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #plt.savefig(\"Breast_Cancer_Decision_Surface_{}depth.pdf\".format(tree_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_surface(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling overfitting in Decision Trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision-tree learners can create overly complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can easily be seen by increasing tree depth to unreasonable values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_surface(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do about overfitting in sklearn? As mentioned, we have several tools at our disposal:\n",
    "- **max_depth**: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. A too high value of maximum depth causes overfitting, whereas a too low value causes underfitting.\n",
    "- **min_samples_leaf**: By specifying a minimum number of samples per leaf, overfitting can be controlled for.\n",
    "- **ccp_alpha**: Cost Complexity (CCP) alpha paramter determining the size of the penalty."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Check the effect of the max_depth and the min_samples_leaf parameters. How would you use them to prevent overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_surface(max_depth=15, min_samples_leaf=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the **cost complexity** as an effective measure in avoiding overfitting. The cost complexity of a tree (CCP(T)) is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "CCP(T) = ERR(Z) + \\alpha L(T)\n",
    "\\end{equation}\n",
    "\n",
    "where ERR(Z) is the total misclassification rate of the terminal nodes and L(T) is the number of terminal nodes of tree T. This type of formula should look familiar, as it closely resembles the regularized regression loss functions we know.\n",
    "\n",
    "To get an idea of what values of $\\alpha$ could be appropriate, `scikit-learn` provides `DecisionTreeClassifier.cost_complexity_pruning_path` that returns the effective alphas (i.e., those that will achieve the next step in complexity reduction) and the corresponding total leaf impurities at each step of the pruning process. As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)\n",
    "\n",
    "# fit decision tree (without limit on max_depth, i.e. tree will grow fully if alpha is set to 0)\n",
    "tree_classifier = DecisionTreeClassifier(random_state=0, \n",
    "                                         criterion=\"gini\")\n",
    "\n",
    "# compute cost_complexity_pruning_path \n",
    "path = tree_classifier.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot cost_complexity_pruning_path\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")  # we remove the last alpha as this corresponds to a tree with only the root node\n",
    "ax.set_xlabel(\"effective alpha\",fontsize=16)\n",
    "ax.set_ylabel(\"total impurity of leaves\",fontsize=16)\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\",fontsize=16)\n",
    "#plt.savefig(\"Determining_Alpha.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a decision tree using the effective alphas. The last value in ccp_alphas is the alpha value that prunes the whole tree, leaving the tree with one node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    tree = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    tree.fit(X_train, y_train)\n",
    "    trees.append(tree)\n",
    "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "      trees[-1].tree_.node_count, ccp_alphas[-1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of this example, we remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node. Here we show that the number of nodes and tree depth decreases as alpha increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = trees[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [tree.tree_.node_count for tree in trees]\n",
    "depth = [tree.tree_.max_depth for tree in trees]\n",
    "fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\",fontsize=16)\n",
    "ax[0].set_ylabel(\"number of nodes\",fontsize=16)\n",
    "ax[0].set_title(\"Number of nodes vs alpha\",fontsize=16)\n",
    "ax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\",fontsize=16)\n",
    "ax[1].set_ylabel(\"depth of tree\",fontsize=16)\n",
    "ax[1].set_title(\"Depth vs alpha\",fontsize=16)\n",
    "fig.tight_layout()\n",
    "#plt.savefig(\"Pruning_effect.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ccp_alphas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could implement a grid search over the identified effective alphas to determine where predictive performance is maximized (feel free to try this out!)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small excursion: Naive Bayes\n",
    "\n",
    "In last week's lecture, you also learned about the Naive Bayes algorithm. It is based on conditional probabilities that you use to calculate the change in probability of class membership. Say, for example, you have an unknown cell structure and want to calculate how likely it is that this cell belongs to the Malignant cells. You can, then, look at the conditional probability of a cell having that specific mean area to predict whether the unknown cell is malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Naive Bayes from sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "point_benign, point_malignant = gnb.theta_ # gives the mean of each feature per class\n",
    "radius_benign, radius_malignant = np.sqrt(gnb.var_) # gives the SD of each feature per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpl_patches\n",
    "benign = mpl_patches.Ellipse(xy=point_benign, width=3 * radius_benign[0], height=3 * radius_benign[1], facecolor='none', edgecolor='b', linewidth=0.5)\n",
    "malignant = mpl_patches.Ellipse(xy=point_malignant, width=3 * radius_malignant[0], height=3 *radius_malignant[1], facecolor='none', edgecolor='r', linewidth=0.5)\n",
    "plt.scatter(point_benign[0], point_benign[1], color='b')\n",
    "plt.scatter(point_malignant[0], point_malignant[1], color='r')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.add_artist(benign)\n",
    "ax.add_artist(malignant)\n",
    "\n",
    "s1 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='M']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='M']['concave points_mean'], marker='x', color='C3', linewidths=0.2)\n",
    "s2 = plt.scatter(cancer_df[cancer_df[\"diagnosis\"]=='B']['area_mean'], cancer_df[cancer_df[\"diagnosis\"]=='B']['concave points_mean'], marker='+', color='C0', linewidths=0.2)    \n",
    "plt.xlim([0,2600])\n",
    "plt.ylim([0,0.21])\n",
    "plt.xlabel(\"Mean Area\",fontsize=16)\n",
    "plt.ylabel(\"Mean Concave Points\",fontsize=16)\n",
    "plt.legend([s1,s2],['Malignant','Benign'],fontsize=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could now evaluate and compare both models using the known cross-validation procedure and classification evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Trees for regression: predicting peak electricity demand\n",
    "\n",
    "We continue with our electric power example from last week which we retieved from PJM from the following link [here](https://dataminer2.pjm.com/feed/hrl_load_metered/definition). The files we are loading are the raw files we downloaded from this source. The final input data for our code is `Pittsburgh_load_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Pittsburgh_load_data.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d.%m.%Y\")\n",
    "df[\"Month\"] = df[\"Date\"].apply(lambda x: x.month)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y vectors\n",
    "Xp = df[\"High_temp\"].values\n",
    "Yp = df[\"MAX\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(Xp, Yp, marker=\"x\")\n",
    "plt.xlabel(\"High Temperature (°C)\")\n",
    "plt.ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already shortly touched upon decision trees for regression in Workshop 8. But let's quickly revisit. We will use the `DecisionTreeRegressor` class in `scikitlearn`to fit and plot a decision tree regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "def plot_tree_regression_line(tree_depth):\n",
    "    # fit regression model (to full data)\n",
    "    Tree_reg = DecisionTreeRegressor(max_depth=tree_depth,criterion=\"squared_error\")\n",
    "    Tree_reg.fit(Xp.reshape((-1,1)), Yp) \n",
    "    \n",
    "    attributes = Tree_reg.tree_\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.scatter(Xp, Yp, marker=\"x\")\n",
    "    plt.plot(np.arange(-18,40,1), Tree_reg.predict(np.arange(-18,40,1).reshape((-1,1))), marker=\"x\", color='C1')\n",
    "    plt.xlabel(\"High Temperature (°C)\", fontsize=16)\n",
    "    plt.ylabel(\"Peak Demand (GW)\", fontsize=16)\n",
    "    \n",
    "    print(\"number of nodes: \", attributes.node_count)\n",
    "    print(\"number of leafs: \", attributes.n_leaves)\n",
    "    \n",
    "    #plt.savefig(\"Peak_Power_Regression_Lines_{}depth.pdf\".format(tree_depth))\n",
    "    return Tree_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = plot_tree_regression_line(tree_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plot_tree(tree_reg, feature_names=['High Temperature'], class_names=['Peak Demand'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ensemble Methods\n",
    "\n",
    "In predictive modeling, “risk” is equivalent to variation (i.e. variance) in prediction error. Ensemble methods are targeted at reducing variance, thus increasing predictive power.\n",
    "The core idea is that by combining the outcomes of individual models, e.g., by taking an average, variance may be reduced. Thus, using an average of two or more predictions can potentially lead to smaller error variance, and therefore better predictive power."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not discuss ensemble methods in detail here, but will limit our discussion to a brief introduction of two very popular tree-based ensemble methods. These are:\n",
    "\n",
    "- XGBoost (a type a boosting method): see [here](https://xgboost.readthedocs.io/en/latest/)\n",
    "- RandomForest: see [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost is an ensemble method that uses **boosting**. While XGBoost is not included in sklearn, there is a very well developed API that can be installed by executing the following command:\n",
    "- `conda install -c conda-forge xgboost`\n",
    "\n",
    "Once you have completed the installation you are good to go. Let us fit a very simple classifier to the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split on breast cancer dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify and fit model\n",
    "import xgboost as xgb\n",
    "xgb_classifier = xgb.XGBClassifier(booster=\"gbtree\")\n",
    "xgb_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(y_test,xgb_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,xgb_classifier.predict(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, there is likely room for improvement as you grid search some of the hyperparameters. However, by just taking the default setting, we already achieve an accuracy score that is comparable to that of the grid-searched decision tree above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forests is a selection of n trees which are trained in parallel. Predictions are made by averaging the outputs across these n trees. Random Forest are most often combined with **bagging**, i.e. different boostrap samples of the training data are used to train the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sepcify and fit model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, \n",
    "                                       bootstrap=True, random_state=42) # we select boostrapp, i.e. we use bagging\n",
    "rf_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(y_test,rf_classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,rf_classifier.predict(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just by taking the default setting, we obtain very good results that are comparable to those of the fully grid-searched decision tree."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('neural-net')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e9c2704faa5823b50c1fba0d53b540a3c6e866c07e353f5f488791d06223338"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

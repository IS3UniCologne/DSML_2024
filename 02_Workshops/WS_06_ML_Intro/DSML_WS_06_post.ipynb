{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DSML Workshop 06` - Introduction to ML\n",
    "\n",
    "In this workshop we start with hands-on machine learning focusing mostly on the linear regression example covered previously in the lecture.\n",
    "\n",
    "Machine learning is a fast-growing field. Recent popularity of machine learning as a topic can largely be explained due to a combination of three different trends: \n",
    "1. Gains in computing power (machine learning, as it grew out of computer science, has always been fundamentally concerned with computational algorithms)\n",
    "1. Massive amounts of available data (the \"raw materials\" for machine learning methods)\n",
    "1. Some notable algorithmic advances that have occurred over the past 30 years \n",
    "\n",
    "To start, though, we will cover some of the most basic algorithms to codify the underlying principles. We will go through the following:\n",
    "- **Task**: Analyzing speed limit violations in Cologne\n",
    "- **Task**: Getting started with ML in Python\n",
    "- **Dataset**: Introducing & preparing Pittsburgh load data\n",
    "- **Predicting peak demand manually**\n",
    "- **Libraries for ML in Python**\n",
    "- **Regression model evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task: Analyzing speed limit violations in Cologne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we really got into hands-on data science using carsharing availability data. Let's put what we learned into practice using data on speed limit violations from August 2022 (the provided datasets are based on this publicly available dataset accessible via [this link](https://offenedaten-koeln.de/dataset/geschwindigkeitsueberwachung-koeln)). The following information are available:\n",
    "- *date*: date of the violation\n",
    "- *time*: time of the violation \n",
    "- *speed*: speed in km/h\n",
    "- *excess_speed*: speed above the allowed speed limit in km/h\n",
    "- *location_id*: identifier of the speed control location\n",
    "\n",
    "Additionally, we have a dataset with further information on the speed control, including:\n",
    "- *location_id*: identifier of the speed control location\n",
    "- *speed_limit*: speed limit in km/h\n",
    "- *district*: abbreviation for city district of speed control location\n",
    "- *street*: street name of speed control location\n",
    "\n",
    "Complete the following steps:\n",
    "1. Import pandas and load the speeding violations dataset (speeding_cgn_2018_08.csv). Is there missing data? How many violations were there in total during the selected period?\n",
    "2. Let's properly format the *date* and *time* columns. Combine them into one column and transform them to datetime format. Extract the hour and day of week, and add them as new columns to the dataframe. Hint: make sure that you specify the correct format when transforming to datetime.\n",
    "3. We want to add the information on the speed control locations contained in *control_locations.csv*. We only want to analyze those violations at control locations for which we have data in control_locations.csv. Load *control_locations.csv* and perform the appropriate merge, so that all relevant information are in one dataframe. Hint: think about the correct setting for the how-parameter in merge.\n",
    "4. Create two plots: first, show boxplots of the number of speeding violations by day of week. Second, show boxplots of the number of speeding violations by hour of day.\n",
    "\n",
    "Feel free to use your knowledge to conduct further analyses on the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "violations = pd.read_csv(\"speeding_cgn_2018_08.csv\")\n",
    "\n",
    "violations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: no missing data\n",
    "violations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of total violations\n",
    "print(\"There were {} total speeding violations in August 2022.\".format(len(violations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine date and time into one column\n",
    "violations[\"timestamp\"] = violations[\"date\"].astype(str) + \" \" + violations[\"time\"]\n",
    "\n",
    "# transform timestamp into datetime format\n",
    "violations[\"timestamp\"] = pd.to_datetime(violations[\"timestamp\"], format=\"%d/%m/%y %H:%M:%S\")\n",
    "\n",
    "# extract hour and day of week\n",
    "violations[\"hour\"] = violations[\"timestamp\"].dt.hour\n",
    "violations[\"weekday\"] = violations[\"timestamp\"].dt.weekday\n",
    "\n",
    "violations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load control_locations.csv\n",
    "locations = pd.read_csv(\"control_locations.csv\")\n",
    "\n",
    "# merge\n",
    "violations_merged = violations.merge(locations, on=\"location_id\", how=\"inner\")\n",
    "\n",
    "violations_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# violations by day of week\n",
    "## group data\n",
    "violations_weekday = violations_merged.groupby([\"date\",\"weekday\"]).count()\n",
    "\n",
    "## plot\n",
    "fig,ax = plt.subplots(figsize=(10,4)) \n",
    "sns.boxplot(x=violations_weekday.index.get_level_values(1), y=violations_weekday[\"speed\"],ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violations by hour of day\n",
    "## group data\n",
    "violations_hour = violations_merged.groupby([\"date\",\"hour\"]).count()\n",
    "\n",
    "## plot\n",
    "fig,ax = plt.subplots(figsize=(10,4)) \n",
    "sns.boxplot(x=violations_hour.index.get_level_values(1), y=violations_hour[\"speed\"],ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task: Getting started with ML in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will start looking at how to do machine learning in Python. Specifically, we will start predicting continuous variables.\n",
    "\n",
    "Let's get a first understanding using a familiar dataset - the tips dataset from Seaborn. Execute the following cell to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we visualized this dataset in an earlier workshop, we noticed that there is a relationship between `total_bill` and `tip` (which is no surprise). Create a scatter plot of these two features with `total_bill` on the x-axis and `tip` on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a restaurant asks you to predict tips based on the total bill of customers. You propose to make predictions based on a simple linear model of the form\n",
    "\\begin{equation}\n",
    "\\mathrm{tip} \\approx \\theta_1 \\cdot \\mathrm{total\\_bill} + \\theta_2\n",
    "\\end{equation}\n",
    "where $\\theta_1$ is the slope of the line and $\\theta_2$ is the intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot you generated above, guess appropriate values for the slope and intercept and assign them to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = 0.2\n",
    "intercept = -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have chosen values for slope and intercept, re-generate the scatter plot from above, but add your linear prediction to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
    "plt.plot([0,50], [slope*0-0.5, slope*50-0.5], color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first observation in our tips dataset has a total bill of 17 dollars and gave a tip of 1 dollar. How far is your prediction for this observation off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "true_tip = 1\n",
    "predicted_tip = slope * 17 - 0.5\n",
    "\n",
    "print(\"Error:\", str(abs(true_tip-predicted_tip)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset: Introducing & preparing Pittsburgh load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to predict what the peak electricity demand will be\n",
    "during the day tomorrow for some area (we'll consider data from the area\n",
    "surrounding Pittsburgh, PA).  This is actually a very important problem from a\n",
    "logistics planning perspective: electricity generators, which for the most part\n",
    "are based upon boiling water to move turbines, cannot turn on instantly, so in\n",
    "order to guarantee that we have enough power to supply a given area, a system\n",
    "operator typically needs to have some excess generation always waiting in the\n",
    "wings. The better we can forecast future demand, the smaller our excess margin\n",
    "can be, leading to increased efficiency of the entire electrical grid.  \n",
    "\n",
    "The power consumption tomorrow depends on many factors: temperature, day of\n",
    "week, season, holiday events, etc., not to mention some inherent randomness\n",
    "that we don't expect to even predict with perfect accuracy.  However, even for\n",
    "someone working in the area, it would be very difficult to come up with a model \n",
    "for electricity demand based soley upon \"first principles\", thinking about the \n",
    "nature of electricity consumption or the devices people may use, in an attempt \n",
    "to predict future consumption.\n",
    "\n",
    "What _is_ easy, however, is simply to collect lots of data about past energy\n",
    "consumption (the system operator serving the Pittsburgh region, PJM, maintains\n",
    "a data set available [here](https://dataminer2.pjm.com/feed/hrl_load_metered/definition)) \n",
    "as well as the past factors that affect consumption, like the past weather for\n",
    "the area. The files we are loading are the raw files we downloaded from this source. \n",
    "The final input data for our code is `Pittsburgh_load_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the data, have a first look at it, and convert the 'Date' column to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Pittsburgh_load_data.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 'AVG', 'MAX', and 'MIN' are in GW, 'Total' in GWh, and 'High_temp' and 'Avg_temp' in degrees Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"],format=\"%d.%m.%Y\")\n",
    "df = df.sort_values(\"Date\")\n",
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we want to build a simple ML model to forecast peak demand.\n",
    "Using the converted Date column, we can have a look at the interaction between the date and the peak demand, which can be found in the 'MAX' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (16,9))\n",
    "\n",
    "# Plotting the data\n",
    "ax.plot(df[\"Date\"],df[[\"MAX\"]])\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we want to look at the summer months only. Let's create a `df_summer` which only contains the months of June, July and August."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create month feature\n",
    "df[\"Month\"] = df[\"Date\"].apply(lambda dt: dt.month)\n",
    "\n",
    "# create df_summer\n",
    "summer_month=[6,7,8]\n",
    "df_summer = df[df[\"Month\"].isin(summer_month)==True]\n",
    "\n",
    "df_summer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us define our dependent (y) and independent (x) variables for peak electricity load prediction\n",
    "xp = df_summer['High_temp']\n",
    "\n",
    "yp = df_summer['MAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot our x and y\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "ax.scatter(xp, yp, marker='x',)\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# saving figures (You can comment-out this line in order not to save figures)\n",
    "#plt.savefig('summer_data_peak_demand.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicting peak demand manually"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, electricity consumption in the summer in Pittsburgh is largely\n",
    "driven by air conditioning, so with increasing high temperature comes increasing\n",
    "electricity demand.  Thus, we may hypothesize that we can form a fairly good prediction of the peak demand using a linear model: that is, we hypothesize that \n",
    "\\begin{equation}\n",
    "\\mathrm{PeakDemand} \\approx \\theta_1 \\cdot \\mathrm{HighTemperature} + \\theta_2\n",
    "\\end{equation}\n",
    "where $\\theta_1$ is the slope of the line and $\\theta_2$ is the intercept term (together called the _parameters_ of the model).  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, just eyeballing the data we might guess that the slope for peak demand data and average demand data is approximately $\\theta_1 = 0.07$ (i.e., we get an increase of 0.07 GW per degree C, which we just arrived at by seeing that the power increased, very approximately, by noticing a total of ~1 GW increase in the range of 14 to 28 degree).  If we further suppose that the average peak demand is about 2.1 GW at 26 degrees C, then we can solve for the intercept term by $0.07*26 + \\theta_2 = 2.1$, or $\\theta_2 = 0.28$.  Note: if this seems ad-hoc to you, don't worry, it is! We will shortly decribe to actually find good values for these parameters in a disciplined way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define theta vector with our estimated values for theta1 and theta2\n",
    "theta = np.array([0.07, 0.28])\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "# add objects to axis\n",
    "ax.scatter(xp, yp, marker='x')\n",
    "xlim, ylim =(plt.gca().get_xlim(), plt.gca().get_ylim())  # gets us the limits of x and y\n",
    "ax.plot(xlim,[theta[0]*xlim[0]+theta[1], theta[0]*xlim[1]+theta[1]], 'C1')\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()\n",
    "#plt.savefig('summer_data_peak_demand_line.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, this model won't fit the data exactly (we can see from the chart that the figure doesn't lie precisely on an exact line), but if we can find slope and intercept terms that fit the data well, then we can simply plug in tomorrow's forecasted high temperature into the equation above and get an estimate of the peak demand tomorrow (ignore the fact, for now, that the high temperature tomorrow is also a prediction - we'll assume we just get this from a reliable source, and domains like weather forecasting are extremely well-studied in practice). This is of course equivalent to just \"finding a point on the line\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding good parameters\n",
    "\n",
    "The question, of course, is how to find \"good\" values for $\\theta_1$ and $\\theta_2$ that fit this data well, so that the line fits the data as \"closely\" as possible. The method we will describe for doing this (which is called _gradient descent_) is probably not the simplest algorithm for finding this fit. In fact, as we will see, there is a very simple closed-form expression that will immediately give us the same solution for the framework we consider here. But gradient descent is an _extremely_ powerful and general algorithm (and _is_ actually quite simple compared to some alternative approaches), and it is no exaggeration to say that gradient descent underlies virtually all modern machine learning. So, with these caveats in mind, let's dive into understanding how we find \"good\" parameters $\\theta_1$ and $\\theta_2$ in a disciplined manner.\n",
    "\n",
    "**Objective functions**:  In order to find good values for the parameters, we need to formally define what \"good\" means in this setting.  This will actually be one of the key questions for machine learning algorithms in general, and different notions of \"goodness\" lead to different algorithms.  Fortunately, there are some very well-studied definitions in this context, and so we have some \"standard\" options that we can try.  The notion that we will consider here captures the idea of the \"squared error\" between the prediction and the actual values. That is, we consider all the days in the plot above, where $\\mathrm{HighTemperature}^{(i)}$ denotes the high temperature and $\\mathrm{PeakDemand}^{(i)}$ denotes the peak demand on day $i$. Since _predicted_ peak demand for day $i$ is equal to\n",
    "\\begin{equation}\n",
    "\\theta_1 \\cdot \\mathrm{HighTemperature} + \\theta_2\n",
    "\\end{equation}\n",
    "we want to make this quantity as close as possible, averaged over all the days, to the true $\\mathrm{PeakDemand}^{(i)}$.  We're going to measure this closeness in terms of the squared difference between the predicted and actual peak demand.  More formally, we would like to minimize the quantity:\n",
    "\\begin{equation}\n",
    "\\frac{1}{\\# \\mathrm{days}} \\sum_{i \\in \\mathrm{days}} \\left ( \\theta_1 \\cdot \\mathrm{HighTemperature}^{(i)} + \\theta_2 - \\mathrm{PeakDemand}^{(i)} \\right )^2 \\equiv E(\\theta)\n",
    "\\end{equation}\n",
    "which we abbreviate as $E(\\theta)$ to emphasize the fact that we are going to be minimizing this error by tuning our $\\theta$ variables.  This is known as the _objective function_ that we are trying to minimize.  A natural question that you may be asking is: why did we choose to measure closeness using this squared difference?  Why not use the average of absolute difference?  Or the maximum absolute difference?  These are good questions, and we'll defer answering them for now, except for saying that we will definitely consider other possibilities later.  The squared error is simply a very common choice, mainly for reasons of mathematical convenience."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data scaling**  We'll shortly see what the gradient descent procedure looks like in our example above.  Before we apply the algorithm, though, we're going to make one small modification to our problem, and _normalize_ the data (both the inputs and the output) before we attempt to run the gradient descent algorithm.  We will see the reason for this more clearly shortly, but the brief reason is that if our slope and intercept terms are on very different \"scales\" (which would be the case here, because of the relatively large input values (units of degrees Celsius) compared to the output values (units of gigawatts)), then we would actually need to take very different step sizes in the two parameters $\\theta_1$ and $\\theta_2$.  This is possible to manually tune in our case, but when we start having many more parameters, it's not feasible.  We thus make our life much easier if we scale all the input and output data to be in the same rough range _before_ running gradient descent (Note: in class we talked about only scaling the input, which also happens to work fine here, but I think it may be easier conceptually to consider the case where we just normalize all the inputs and outputs in the same manner).\n",
    "\n",
    "We can re-scale data in a number of ways, but a simple strategy is just to translate and scale the coordinates such that the values vary between zero and one in our dataset.  This can be easily achieved by the transformation\n",
    "\\begin{equation}\n",
    "\\tilde{x}^{(i)} = \\frac{x^{(i)} - \\min(x)}{\\max(x) - \\min(x)}\n",
    "\\end{equation}\n",
    "and similarly for $\\tilde{y}^{(i)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nor = (xp-min(xp))/(max(xp)-min(xp))\n",
    "\n",
    "y_nor = (yp-min(yp))/(max(yp)-min(yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the difference after re-scaling\n",
    "fig, axes = plt.subplots(1,2,figsize = (16,9))\n",
    "\n",
    "axes[0].scatter(xp, yp, marker='x')\n",
    "axes[0].set_xlabel(\"Temperature\")\n",
    "axes[0].set_ylabel(\"Demand\")\n",
    "\n",
    "axes[1].scatter(x_nor, y_nor, marker = 'x')\n",
    "axes[1].set_xlabel(\"Re-scaled Temperature\")\n",
    "axes[1].set_ylabel(\"Re-scaled Demand\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left plot looks identical to the right plot, of course, except that the units no longer correspond to traditional quantities like degrees Celsius or gigawatts, but just some linear transformation of these units."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing gradient descent\n",
    "\n",
    "Now let's look at the gradient descent algorithm, which we have derived mathematically in previous lectures.  This will initialize $\\theta_1$ and $\\theta_2$ to zero and repeatedly update them according to the partial derivative rules.  We will use the step size (also known as learning rate) $\\alpha=0.5$, and print out the value of $\\theta$ per each iteration.\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_j  := \\theta_j  − \\alpha \\sum_{j=1}^m (\\sum_{i=1}^n \\theta_j x_j^{(i)} - y^{(i)})x_j^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "Note: The above simplified equation omits the number of days (n) and the *2 term which results from partial differentiation and groups them into the alpha term for easier representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([0., 0.])\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "for t in range(20):\n",
    "    print(\"Iteration {}: \".format(t), theta)\n",
    "    theta_old = theta.copy()\n",
    "    # partial derivative theta1: 2*sum(((theta1*x + theta2)-y)*x)\n",
    "    # term 2/N added here (usully included in alpha term), this is just a constant so could also be omitted\n",
    "    theta[0] -= alpha/len(xp) * 2 * sum((theta_old[0] * x_nor + theta_old[1] - y_nor)*x_nor)\n",
    "    \n",
    "    # partial derivative theta2: 2*sum(((theta1*x + theta2)-y))\n",
    "    # term 2/N added here (usully included in alpha term), this is just a constant so could also be omitted\n",
    "    theta[1] -= alpha/len(xp) * 2 * sum((theta_old[0] * x_nor + theta_old[1] - y_nor) ) # this is the intercept with a slightly different partial derivative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what this looks like in a couple different ways.  First, let's look at what our line looks like during different iterations of gradient descent.  For this purpose, we'll wrap the above in a simple function that takes `iters` iterations of gradient descent (note that we can of course get all these plots within a single run of gradient descent, but we'll just use multiple calls to this function for illustration purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(iters, alpha):\n",
    "    theta = np.array([0., 0.])\n",
    "    for t in range(iters):\n",
    "        theta_old = theta.copy()\n",
    "        theta[0] -= alpha/len(xp) * 2 * sum((theta_old[0] * x_nor + theta_old[1] - y_nor)*x_nor)\n",
    "        theta[1] -= alpha/len(xp) * 2 * sum((theta_old[0] * x_nor + theta_old[1] - y_nor) )\n",
    "    return theta\n",
    "\n",
    "def plot_fit(theta):\n",
    "    \n",
    "    Error = sum((theta[0]*x_nor + theta[1] - y_nor)**2) # simple least squares error, which underlies our OLS example\n",
    "    \n",
    "    # compute partial derivative (i.e. gradient) for theta 1 and 2\n",
    "    def_theta1 = sum((theta[0] * x_nor + theta[1] - y_nor)*x_nor)\n",
    "    def_theta2 = sum((theta[0] * x_nor + theta[1] - y_nor))\n",
    "    \n",
    "    # plot\n",
    "    \n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.scatter(x_nor, y_nor, marker = 'x')\n",
    "    plt.xlabel(\"Nomalized Temperature\")\n",
    "    plt.ylabel(\"Normalized Demand\")\n",
    "    xlim, ylim =(plt.gca().get_xlim(), plt.gca().get_ylim())\n",
    "    \n",
    "    plt.plot(xlim, [theta[0]*xlim[0]+theta[1], theta[0]*xlim[1]+theta[1]], 'C1')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    print('Theta = ', theta, 'Error = ',Error,'def_theta1 = ',def_theta1, 'def_theta2 = ', def_theta2 )\n",
    "    #plt.savefig('gradient decent '+str(theta[0])+'.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(0, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(1, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(2, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(3, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(4, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(5, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(10, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(100, alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(1000, alpha=0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error versus iteration\n",
    "\n",
    "We can also look at the average error versus iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_err(iters):\n",
    "    err = []\n",
    "    theta = np.array([0., 0.])\n",
    "    alpha = 0.5\n",
    "    for t in range(iters):\n",
    "        theta_old = theta.copy()\n",
    "        err.append(np.mean((theta_old[0] * x_nor + theta_old[1] - y_nor)**2))\n",
    "        theta[0] -= alpha/len(xp) * 2 * sum((theta_old[0] * x_nor + theta_old[1] - y_nor)*x_nor)\n",
    "        theta[1] -= alpha/len(xp) * 2 * sum((theta_old[0] * x_nor + theta_old[1] - y_nor) )\n",
    "    return np.array(err)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(np.arange(0,50), gradient_descent_err(50))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the effect of the step size (learning rate)\n",
    "\n",
    "When using gradient descent, one of the hyperparameters we need to set is the step size (or learning rate) denoted by $\\alpha$ in the gradient descent updating formula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_j  := \\theta_j  − \\alpha \\sum_{j=1}^m (\\sum_{i=1}^n \\theta_j x_j^{(i)} - y^{(i)})x_j^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "To understand the effect of different choices for $\\alpha$, let us visualize the gradient descent iterations from our example above in a different way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create contours of loss function using grid of theta1 and theta2 values and generating the corresponding error for all combinations\n",
    "theta1_vals = np.linspace(0, 1.6, 100)\n",
    "theta2_vals = np.linspace(-0.3, 0.5, 100)\n",
    "se_vals = np.zeros(shape=(theta1_vals.size, theta2_vals.size)) # placeholder to be populated with errors below\n",
    "for i, val_2 in enumerate(theta2_vals):\n",
    "    for j, val_1 in enumerate(theta1_vals):\n",
    "        se_vals[i, j] = sum((val_1*x_nor + val_2 - y_nor)**2)\n",
    "\n",
    "# save theta1 and theta2 values of first 50 gradient descent iterations\n",
    "gd_theta1s = []\n",
    "gd_theta2s = []\n",
    "for i in range(50):\n",
    "    gd_theta1s.append(gradient_descent(i, alpha=0.5)[0])\n",
    "    gd_theta2s.append(gradient_descent(i, alpha=0.5)[1])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "CS = plt.contour(theta1_vals, theta2_vals, se_vals, levels=np.arange(0,510, 10), linewidths=2, colors=\"steelblue\")\n",
    "#plt.clabel(CS, inline=1, fontsize=8)\n",
    "plt.plot(gd_theta1s, gd_theta2s, color=\"orange\", linewidth=2, marker=\"x\", markersize=8)\n",
    "plt.title(\"Contour Plot of Gradient Descent (50 iterations)\", fontsize=14)\n",
    "plt.xlabel(\"theta1\", fontsize=12)\n",
    "plt.ylabel(\"theta2\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's illustrate what happens if we set $\\alpha$ to, for example, 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save theta1 and theta2 values of first 50 gradient descent iterations\n",
    "gd_theta1s = []\n",
    "gd_theta2s = []\n",
    "for i in range(50):\n",
    "    gd_theta1s.append(gradient_descent(i, alpha=0.6)[0])\n",
    "    gd_theta2s.append(gradient_descent(i, alpha=0.6)[1])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "CS = plt.contour(theta1_vals, theta2_vals, se_vals, levels=np.arange(0,510, 10), linewidths=2, colors=\"steelblue\")\n",
    "#plt.clabel(CS, inline=1, fontsize=8)\n",
    "plt.plot(gd_theta1s, gd_theta2s, color=\"orange\", linewidth=2, marker=\"x\", markersize=8)\n",
    "plt.title(\"Contour Plot of Gradient Descent (50 iterations)\", fontsize=14)\n",
    "plt.xlabel(\"theta1\", fontsize=12)\n",
    "plt.ylabel(\"theta2\", fontsize=12)\n",
    "plt.xlim([0, 1.6])\n",
    "plt.ylim([-0.3, 0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as if this was a smart move. We now arrived at a lower loss after 50 iterations compared to $\\alpha = 0.5$, so let's increase our learning rate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save theta1 and theta2 values of first 50 gradient descent iterations\n",
    "gd_theta1s = []\n",
    "gd_theta2s = []\n",
    "for i in range(50):\n",
    "    gd_theta1s.append(gradient_descent(i, alpha=0.7)[0])\n",
    "    gd_theta2s.append(gradient_descent(i, alpha=0.7)[1])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "CS = plt.contour(theta1_vals, theta2_vals, se_vals, levels=np.arange(0,510, 10), linewidths=2, colors=\"steelblue\")\n",
    "#plt.clabel(CS, inline=1, fontsize=8)\n",
    "plt.plot(gd_theta1s, gd_theta2s, color=\"orange\", linewidth=2, marker=\"x\", markersize=8)\n",
    "plt.title(\"Contour Plot of Gradient Descent (50 iterations)\", fontsize=14)\n",
    "plt.xlabel(\"theta1\", fontsize=12)\n",
    "plt.ylabel(\"theta2\", fontsize=12)\n",
    "plt.xlim([0, 1.6])\n",
    "plt.ylim([-0.3, 0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot looks interesting. It appears as if we are still converging, but not necessarily faster compared to $\\alpha = 0.5$ or $\\alpha = 0.6$. What happens if we still increase the step size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save theta1 and theta2 values of first 50 gradient descent iterations\n",
    "gd_theta1s = []\n",
    "gd_theta2s = []\n",
    "for i in range(50):\n",
    "    gd_theta1s.append(gradient_descent(i, alpha=0.8)[0])\n",
    "    gd_theta2s.append(gradient_descent(i, alpha=0.8)[1])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "CS = plt.contour(theta1_vals, theta2_vals, se_vals, levels=np.arange(0,510, 10), linewidths=2, colors=\"steelblue\")\n",
    "#plt.clabel(CS, inline=1, fontsize=8)\n",
    "plt.plot(gd_theta1s, gd_theta2s, color=\"orange\", linewidth=2, marker=\"x\", markersize=8)\n",
    "plt.title(\"Contour Plot of Gradient Descent (50 iterations)\", fontsize=14)\n",
    "plt.xlabel(\"theta1\", fontsize=12)\n",
    "plt.ylabel(\"theta2\", fontsize=12)\n",
    "plt.xlim([0, 1.6])\n",
    "plt.ylim([-0.3, 0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At $\\alpha = 0.8$, it looks like we are diverging - we are \"overshooting the target\" and actually moving further away from the lowest error with each iteration. As you can see, choosing an appropriate learning rate is crucial to balance the tradeoff between converging very slowly and not converging at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the answer back to the original coordinates\n",
    "\n",
    "Fortunately, we don't need to resort to solving the system in the original coordinates, we can simply solve on our normalized data and then find the corresponding equations for the original data.  Specifically, since our model gives the approximation\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\tilde{y} & \\approx \\tilde{x} \\cdot \\theta_1 + \\theta_2\\\\\n",
    "\\Longrightarrow \\;\\; \\frac{y-a}{b} & \\approx \\frac{x-c}{d} \\cdot \\theta_1 + \\theta_2 \\\\\n",
    "\\Longrightarrow \\;\\; y-a & \\approx ((x-c)\\cdot \\theta_1)/d)\\cdot b + b \\theta_2 \\\\\n",
    "\\Longrightarrow \\;\\; y & \\approx x \\cdot (b \\theta_1/d) + b \\theta_2 + a - c b \\theta_1/d\\\\\n",
    "\\Longrightarrow \\;\\; y & \\approx x \\cdot \\hat{\\theta}_1 + \\hat{\\theta}_2\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "a = \\min_i y^{(i)}, \\;\\; b = \\max_i y^{(i)} - \\min_i y^{(i)}, \\;\\; c = \\min_i x^{(i)}, \\;\\; d = \\max_i x^{(i)} - \\min_i x^{(i)}, \n",
    "\\end{equation}\n",
    "and where we define\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{\\theta}_1 & = b \\theta_1/d \\\\\n",
    "\\hat{\\theta}_2 & = b \\theta_2 + a - c \\cdot(b \\theta_1/d).\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "That might seem like a lot, but all it's saying is that there is an easy formula to convert between the solution we get for the normalized data and the unnormalized data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get theta 1 and theta 2 from gradient descent\n",
    "theta = gradient_descent(1000, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return thetas on normalized scale\n",
    "theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above expression, let's re-scale the theta parameters to the original scale by creating a new array entitled `theta_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a, b, c, d\n",
    "a = min(yp)\n",
    "b = max(yp)-min(yp)\n",
    "c = min(xp)\n",
    "d = max(xp) - min(xp)\n",
    "\n",
    "# use formula from above to re-scale\n",
    "theta_1 = b*theta[0]/d\n",
    "theta_2 = b*theta[1] + a - c*(b*theta[0]/d)\n",
    "\n",
    "#combine into single array\n",
    "theta_hat = np.array([theta_1,theta_2])\n",
    "\n",
    "theta_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the derived linear regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "\n",
    "ax.scatter(xp, yp, marker='x')\n",
    "\n",
    "xlim, ylim =(plt.gca().get_xlim(), plt.gca().get_ylim())\n",
    "ax.plot(xlim, [theta_hat[0]*xlim[0]+theta_hat[1], theta_hat[0]*xlim[1]+theta_hat[1]], 'C1')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "print(theta, theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Libraries for ML in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we conclude with some information about the types of libraries we will use to run machine learning algorithms in Python.  Although there are a number of machine learning packages available, by far the most popular Python library for general-purpose \"classical\" machine learning (this is in contrast to packages focused specificially on deep learning, such as [TensorFlow](http://www.tensorflow.org) and [Keras](https://keras.io/getting_started/)) is the [scikit-learn](http://scikit-learn.org/) library.  Scikit Learn is a general purpose machine learning library with a number of common machine learning algorithms built in.\n",
    "\n",
    "One important note, however, is that (despite some ongoing efforts to make it more scalable), scikit-learn is still best suited for small to medium-scale problems (say with ~10,000s of examples and ~1,000s of features). For these sized problems, most of the algorithms contained in the library will work reasonably fast, and the library has the advantage that one can train many different types of algorithms all with the same interface. However, if you have datasets that are much larger than this, then the algorithms start to get fairly slow compared to other more specialized libraries, and you are likely better off using an alternative library.\n",
    "\n",
    "Another important caveat, and this is one that sadly often gets ignored, is that unlike other software libraries, you _need_ to have some (even just basic) understanding of what the algorithms do in order to use scikit-learn effectively.  This is because virtually all algorithms will have some substantial number of hyperparameters, settings to the algorithm that can drastically affect performance (and really, affect _all_ the underlying aspects of the algorithm itself, the hypothesis, loss, and optimization problem).  Sadly, a surprisingly large number of the statements people make about data science techniques seem less about the actual algorithms and more about whatever default settings scikit-learn happens to have for each algorithm.  This is why you get people saying things like \"support vector machines worked better than neural networks for this problem\", which is a completely meaningless statement unless you know _what sort_ of support vector machine, and _what architecture_ neural network was used.  Maybe in 10 years we will be at a place where the ML algorithms truly are \"self-contained\", and practitioners don't need to know anything about the underlying algorithms to get good performance (certainly, some researchers and companies are attempting to develop tools that move in this direction). But for the vast majority of tasks, we are still far away from this point, and you do absolutely need to understand the algorithms to get reasonable performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression in scikit-learn\n",
    "\n",
    "Let's look at how we specify a model, fit it to data, and make predictions in scikit-learn. These three tasks form the common usage pattern for most interactions with scikit-learn. Let's first prepare our data. Note that scikit-learn by default will fit a separate intercept term for linear regression models, so we don't include the all-ones entry in our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_summer[\"High_temp\"].values.reshape((-1,1)) # if we pass a 1-feature array we need to re-shape it! This is not required for multi-dimensional arrays\n",
    "y = df_summer[\"MAX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import and initialize our model. In general, scikit-learn has a different class for each different type of learning algorithm. In this case, we are importing the LinearRegression class. When we initialize the class, we pass various parameters to the constructor. In this case, we are specifying that we will fit an intercept term (i.e., we will not pass it as an explicit feature)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `scikit-learn` also has a pre-implemented scaling functionality such as the `StandardScaler` class ([see here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)). For convenience we will not scale the data here. Gradient descent is still able to find an optimal solution. For higher dimensional inputs, however, scaling is highly recommended. We will cover this in future workshops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "linear_model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we create this class we haven't actually passed any data to the system.  This is the standard interface for scikit-learn classes: the constructor just initializes the hyperparameters of the model, and when we actually want to fit it to data, we call the `model.fit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "linear_model.fit(X, y)\n",
    "print(linear_model.coef_, linear_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to own implementation (at 1000 iteration of gradient descent)\n",
    "theta_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we want to make a prediction on a new data point, we call the model.predict() function, passing in the feature values for the new point we want to predict. In the following example, we would be predicting what the peak demand would be given a 25 degree peak temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on new data\n",
    "Xnew = np.array([[25]]) #predict peak load at 25 deg c\n",
    "\n",
    "print(linear_model.predict(Xnew))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can pass multiple points to .predict() at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = np.array([[25],[23]])\n",
    "print(linear_model.predict(Xnew))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit-Learn Library for plotting a linear regression line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's repeat our estimation procedure\n",
    "lr = LinearRegression(fit_intercept=True) # initialize\n",
    "lr.fit(xp.values.reshape(-1,1), yp) # train\n",
    "model_pred = lr.predict(xp.values.reshape(-1,1)) # predict on original x values\n",
    "\n",
    "# plot\n",
    "fig,ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "ax.scatter(xp, yp, marker=\"x\")\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "\n",
    "ax.plot(xp, model_pred, c='C2')\n",
    "ax.legend(['Squared loss fit','Observed days'])\n",
    "#plt.savefig('summer data_peak demand_line.pdf')\n",
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression model evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_summer[\"MAX\"]\n",
    "y_pred = linear_model.predict(df_summer[\"High_temp\"].values.reshape((-1,1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Root) Mean Squared Error ((R)MSE)\n",
    "The MSE calculates the mean of the squared residuals. As the fitting of the linear regression model is (typically) carried out by minimizing the squared error (see loss function in lecture slides), the MSE corresponds directly to the model optimization. The unit of the MSE is the target unit squared, i.e. in the above example using energy consumption in GW, the unit of the MSE is $(GW)^2$. As the residuals (the difference of prediction and observed values) are squared, large deviations are penalized stronger than small deviations. Therefore, the weight of outliers increases using MSE as a metric. This is useful in applications where small prediction errors are not important but large errors have to be avoided.\n",
    "\n",
    "As the squared unit is hard to interpret, the RMSE can be used instead of the MSE. The RMSE is just the square root of the MSE, meaning that it is monotonic with respect to the MSE - a bigger MSE leads to a bigger RMSE and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error:\",mean_squared_error(y_true, y_pred),\"(GW)^2\")\n",
    "print(\"Root Mean Squared Error:\",mean_squared_error(y_true, y_pred)**0.5,\"GW\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE can be easily interpreted as it can be set into relation with the ca. 1.8 GW average demand mean. Generally, MSE and RMSE always have to be assessed given the model under evaluation. A MSE or RMSE with a certain value has no meaning unless the order of magnitude of the dataset is known (with an exception of the very unlikely case that the MSE is zero)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "Just as the RMSE, the MAE is easy to interpret. It is just the mean absolute value of the error term. When there are no or little major outliers, RMSE and MAE often have the same order of magnitude, with the MAE always being smaller than the RMSE. While a linear regression is typically fitted using the least squares method, the MAE can still be a valuable metric. It is applicable when large errors are not disproportionately worse than smaller errors. For example, when prediciting monetary values, an error of 100 USD might always be twice as bad as an error of 50 USD and so on. If this is the case, the MAE can actually be the more suitable error metric. Like the RSME, the MAE only makes sense when knowing the order of magnitude of the predicted values. On its own, it does not imply good or bad model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Absolute Error:\",mean_absolute_error(y_true, y_pred),\"GW\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient of Determination ($R^2$)\n",
    "The coefficient of determination (typically pronounced as \"R squared\"), is a metric of how well a model explains variance seen in the data. $R^2$ indicates the ratio of the explained and the overall variance (given some assumptions that we will not discuss here). Its value is always between 0 and 1. It can therefore be used as a means of comparison not only between regression methods, but also between completely different datasets - or even without knowing the data at all. For example, a value of 0.9 is always good, a value of 0.1 is always bad. Still, what threshold you would define as a good model fit depends on the domain of application. If you expect a high degree of randomness in your data, it is harder to explain your variance using a predictor. If you expect your data to be highly deterministic, then it should be easily explainable using suitable features and prediction methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient of determination:\",r2_score(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other metrics\n",
    "Scikit-learn does include other metrics that are not used as often as the ones explained above. To read up on those, just visit Scikit-learn's website. You can also define functions to calculate your own error metrics - depending on the application and with sound reasoning. E.g., it could make sense to weigh negative deviations higher than positive deviations, or the relative error instead of a form of absolute error could be decisive. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IMPORTANT: Note that the metrics calculated above represent the training loss! They do not say anything about predictive performance!__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[Model Evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0e5296c3657c6b7a86a9bab3436e28ffbdd8356439efa15fab08846068601a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

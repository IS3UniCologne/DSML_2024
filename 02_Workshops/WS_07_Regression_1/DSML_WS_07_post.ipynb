{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DSML Workshop 07` - Multiple Linear Regression and Polynomial Regression\n",
    "\n",
    "In this workshop we continue with hands-on machine learning focusing mostly on the \"non-linear\" regression example (polynomial regression) covered previously in the lecture.\n",
    "\n",
    "This workshop is structured as follows: \n",
    "1. **Task**: Predicting electricity demand\n",
    "1. **Task**: Introduction to advanced regression\n",
    "1. **Using multiple features in a linear regression**\n",
    "1. **Non-linear modeling and polynomial regression**\n",
    "1. **Overfitting & cross-validation**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task: Predicting electricity demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we used electricity load data from Pittsburgh to predict peak electricity demand based on temperature. Using the same dataset, put what you have learned together to predict average electricity demand (`AVG`) based on average temperature (`Avg_temp`) **for the summer months (June, July, August)**.\n",
    "\n",
    "More specifically, do the following:\n",
    "- Load and prepare the `df_summer` dataframe for analysis.\n",
    "- Fit a linear regression by using the `scikit-learn` library for average temperature and average demand data.\n",
    "- Plot the data and fitted line. Annotate your graph appropriately.\n",
    "- Present relevant test metrics to quantify the __training__ loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "df = pd.read_csv(\"Pittsburgh_load_data.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"],format=\"%d.%m.%Y\")\n",
    "df = df.sort_values(\"Date\")\n",
    "df[\"Month\"] = df[\"Date\"].apply(lambda dt: dt.month)\n",
    "summer_month=[6,7,8]\n",
    "df_summer = df[df[\"Month\"].isin(summer_month)==True]\n",
    "\n",
    "df_summer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target and feature vectors\n",
    "Xavg = np.array(df_summer[\"Avg_temp\"]).reshape(-1,1)\n",
    "yavg = np.array(df_summer[\"AVG\"])\n",
    "\n",
    "# initialize linear model\n",
    "lin_model = LinearRegression()\n",
    "\n",
    "# fit model\n",
    "lin_model.fit(Xavg,yavg)\n",
    "\n",
    "# predict training data\n",
    "y_pred_avg = lin_model.predict(Xavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "plt.scatter(Xavg, yavg, marker=\"x\", label='Observed Days')\n",
    "plt.xlabel(\"Average Temperature (Â°C)\")\n",
    "plt.ylabel(\"Average Demand (GW)\")\n",
    "\n",
    "plt.plot(Xavg, y_pred_avg, c='C2', label='Squared loss fit')\n",
    "\n",
    "xlim, ylim =(plt.gca().get_xlim(), plt.gca().get_ylim())\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# present test_metrics\n",
    "print(\"Mean Absolute Error:\",mean_absolute_error(yavg, y_pred_avg),\"GW\")\n",
    "print(\"Root Mean Squared Error:\",mean_squared_error(yavg, y_pred_avg)**0.5,\"GW\")\n",
    "print(\"Coefficient of determination:\",r2_score(yavg, y_pred_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task: Introduction to advanced regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will continue our regression journey and consider some more complex modeling techniques.\n",
    "\n",
    "Let's once again use the familiar tips dataset from Seaborn. Execute the following cell to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the introductory task to linear regression, we manually constructed a regression line to predict the tip based on the total bill using the following simple linear model:\n",
    "\\begin{equation}\n",
    "\\mathrm{tip} \\approx \\theta_1 \\cdot \\mathrm{total\\_bill} + \\theta_2\n",
    "\\end{equation}\n",
    "where $\\theta_1$ is the slope of the line and $\\theta_2$ is the intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the linear regression using scikit learn to find the optimal values for the parameters and plot the regression line on top of a scatter plot of the observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and y\n",
    "X = tips[\"total_bill\"].values.reshape((-1,1))\n",
    "y = tips[\"tip\"]\n",
    "\n",
    "# initialize model\n",
    "lin_mod = LinearRegression()\n",
    "\n",
    "# train model\n",
    "lin_mod.fit(X, y)\n",
    "\n",
    "# make predictions using model\n",
    "model_pred = lin_mod.predict(X)\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(X, y, marker=\"x\")\n",
    "plt.xlabel(\"Total bill\")\n",
    "plt.ylabel(\"Tip\")\n",
    "plt.plot(X, model_pred, c='C2')\n",
    "plt.legend(['Observed values', 'Squared loss fit'])\n",
    "print(lin_mod.coef_, lin_mod.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One obvious way to improve our predictions would be to add another feature. For example, are tips different when people eat for lunch compared to dinner? Add the `time` feature to your regression by doing the following:\n",
    "- Recode the `time` feature so that \"Lunch\" is 0 and \"Dinner\" is 1.\n",
    "- Define your feature matrix `X` to include both `total_bill` and `time_recoded`.\n",
    "- Re-run your linear regression.\n",
    "- Plot a scatter plot with the observed lunch tips and dinner tips in different colors, as well as the respective regression lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode time feature\n",
    "tips[\"time_recoded\"] = tips[\"time\"].map({\"Lunch\":0, \"Dinner\":1})\n",
    "\n",
    "# define X\n",
    "X_2D = tips[[\"total_bill\", \"time_recoded\"]]\n",
    "\n",
    "# re-run regression\n",
    "lin_mod_2D = LinearRegression()\n",
    "lin_mod_2D.fit(X_2D, y)\n",
    "\n",
    "# create two feature vectors for lunch and dinner\n",
    "X_lunch = X_2D[X_2D[\"time_recoded\"]==0]\n",
    "X_dinner = X_2D[X_2D[\"time_recoded\"]==1]\n",
    "\n",
    "# predict lunch and dinner separately (using the same model!)\n",
    "y_pred_lunch = lin_mod_2D.predict(X_lunch)\n",
    "y_pred_dinner = lin_mod_2D.predict(X_dinner)\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(X_lunch[\"total_bill\"], \n",
    "            tips[tips[\"time_recoded\"]==0][\"tip\"], \n",
    "            marker=\"+\", label=\"Observed Lunch\")\n",
    "\n",
    "plt.scatter(X_dinner[\"total_bill\"], \n",
    "            tips[tips[\"time_recoded\"]==1][\"tip\"], \n",
    "            marker=\"x\", label=\"Observed Dinner\")\n",
    "\n",
    "plt.plot(X_lunch[\"total_bill\"], \n",
    "         y_pred_lunch, \n",
    "         label=\"Lunch Prediction\")\n",
    "\n",
    "plt.plot(X_dinner[\"total_bill\"], \n",
    "         y_pred_dinner, \n",
    "         label=\"Dinner Prediction\")\n",
    "\n",
    "plt.xlabel(\"Total bill\")\n",
    "plt.ylabel(\"Tip\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using multiple features in a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load needed libraries for workshop\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap from last workshop: predicting peak electrical power\n",
    "\n",
    "We will again work with our electrical power example from last week, which we retrieved from PJM via the following link [here](https://dataminer2.pjm.com/feed/hrl_load_metered/definition). The files we are loading are the raw files we downloaded from this source. The final input data for our code is `Pittsburgh_load_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Pittsburgh_load_data.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d.%m.%Y\")\n",
    "df[\"Month\"] = df[\"Date\"].apply(lambda x: x.month)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall from last week's workshop**: We fitted a linear model to the summer period of our electricity data. To do so, we used `scikit learn` and obtained the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to summer month\n",
    "df_summer = df[(df[\"Month\"] > 5) & (df[\"Month\"] < 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_summer[\"High_temp\"].values.reshape((-1,1)) # remember: if we pass a 1-feature array we need to re-shape it!\n",
    "y = df_summer[\"MAX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "lin_mod = LinearRegression()\n",
    "\n",
    "# train model\n",
    "lin_mod.fit(X, y)\n",
    "\n",
    "# make predictions using model\n",
    "model_pred = lin_mod.predict(X)\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(X, y, marker=\"x\")\n",
    "plt.xlabel(\"High Temperature (Â°C)\")\n",
    "plt.ylabel(\"Peak Demand (GW)\")\n",
    "plt.plot(X, model_pred, c='C2')\n",
    "plt.legend(['Observed days', 'Squared loss fit'])\n",
    "print(lin_mod.coef_, lin_mod.intercept_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-dimensional linear regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we now incorporate multiple features into our model? Essentially, all we need to do is to pass a larger feature vector to our model. \n",
    "\n",
    "To see this, let us again consider the example of electricity demand. We expect consumption behavior to be different on weekends compared to working days. **Can you think of the key reasons why this might be the case?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a boolean indicator feature entitled `IsWeekday` that returns True if the day is a weekday and False if it falls on a weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember the features of our df_summer dataframe\n",
    "df_summer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a weekday column to our data frame (dt.weekday() might be helpful here)\n",
    "df_summer[\"Weekday\"] = df_summer[\"Date\"].apply(lambda dt: dt.weekday())\n",
    "\n",
    "# create IsWeekday feature\n",
    "df_summer[\"IsWeekday\"] = df_summer[\"Weekday\"].apply(lambda x: 1 if x<=4 else 0)\n",
    "\n",
    "# inspect dataframe\n",
    "df_summer.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, you could also define you own function\n",
    "\n",
    "def weekday_check (dt):\n",
    "    \n",
    "    day_number = dt.weekday()\n",
    "    \n",
    "    if day_number <=4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#df_summer[\"IsWeekday\"] = df_summer[\"Date\"].apply(lambda dt: weekday_check(dt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the results (this time using seaborn for convenience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "sns.scatterplot(x=df_summer[\"High_temp\"], y=df_summer[\"MAX\"],hue=df_summer[\"IsWeekday\"], palette=\"mako\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? Is this in line with you expectation?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train a new linear model with an additional feature for weekdays. As mentioned, all we need to do is to pass a larger feature vector to our linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_summer[[\"High_temp\", \"IsWeekday\"]] # remember: since we now have more than one dimension, we do not need to reshape\n",
    "y = df_summer[[\"MAX\"]]\n",
    "lin_mod_day = LinearRegression()\n",
    "lin_mod_day.fit(X,y)\n",
    "print(lin_mod_day.coef_, lin_mod_day.intercept_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Note that we have now added a second element to our coefficient vector. Can you interpret the results?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let us plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two feature vectors for weekdays and weekends\n",
    "X_weekday = X[X[\"IsWeekday\"]==1]\n",
    "X_weekend = X[X[\"IsWeekday\"]==0]\n",
    "\n",
    "# predict weekdays and weekends separately (using the same model!)\n",
    "y_pred_weekday = lin_mod_day.predict(X_weekday)\n",
    "y_pred_weekend = lin_mod_day.predict(X_weekend)\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(X_weekday[\"High_temp\"], \n",
    "            df_summer[df_summer[\"IsWeekday\"]==1][\"MAX\"], \n",
    "            marker=\"+\", label=\"Observed Weekdays\")\n",
    "\n",
    "plt.scatter(X_weekend[\"High_temp\"], \n",
    "            df_summer[df_summer[\"IsWeekday\"]==0][\"MAX\"], \n",
    "            marker=\"x\", label=\"Observed Weekends\")\n",
    "\n",
    "plt.plot(X_weekday[\"High_temp\"], \n",
    "         y_pred_weekday, \n",
    "         label=\"Weekday Prediction\")\n",
    "\n",
    "plt.plot(X_weekend[\"High_temp\"], \n",
    "         y_pred_weekend, \n",
    "         label=\"Weekend Prediction\")\n",
    "\n",
    "plt.xlabel(\"High Temperature (Â°C)\")\n",
    "plt.ylabel(\"Peak Demand (GW)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has the inclusion of an additional feature improved training performance? Let's look at MAE, RMSE and $R^2$ of both model setups.\n",
    "\n",
    "**Caution**: This is the training error! Do not confuse it with actual model predictive performance, which can only be evaluated on unseen data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define y_true, X_1d as the reshaped x vector containing High_temp, and X2_d containing both High_temp and IsWeekday\n",
    "y_true = df_summer[\"MAX\"]\n",
    "X_1d = df_summer[\"High_temp\"].values.reshape((-1,1)) # need to reshape 1d vector\n",
    "X_2d = df_summer[[\"High_temp\",\"IsWeekday\"]]\n",
    "\n",
    "# make predictions using the fitted models (lin_mod for 1d and lin_mod_day for 2d)\n",
    "y_pred_1d = lin_mod.predict(X_1d)\n",
    "y_pred_2d = lin_mod_day.predict(X_2d)\n",
    "\n",
    "# print results\n",
    "print(\"MAE 1D:\", mean_absolute_error(y_true,y_pred_1d))\n",
    "print(\"MAE 2D:\", mean_absolute_error(y_true,y_pred_2d))\n",
    "\n",
    "print(\"RMSE 1D:\", mean_squared_error(y_true,y_pred_1d)**0.5)\n",
    "print(\"RMSE 2D:\", mean_squared_error(y_true,y_pred_2d)**0.5)\n",
    "\n",
    "print(\"R2 1D:\", r2_score(y_true, y_pred_1d))\n",
    "print(\"R2 2D:\", r2_score(y_true,y_pred_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Non-linear modeling (polynomial regression)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between high temperature and electrical demand is well-modeled by a linear function during the summer months, when (at least in Pittsburgh), electricity is dominated by air conditioners (so that with higher temperatures comes higher consumption). \n",
    "\n",
    "However, this is clearly not the case for the entire year. Indeed, if our previous linear model is believed to be applicable in general, then with lower temperatures we would continue to have lower and lower consumption (until, at some point, weâd start generating electricity). Naturally, this is not the case, and if we instead consider the entire year of high temperature and peak demand, or average temperature and average demand, then a different picture emerges."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the following notation:\n",
    "\n",
    "- xp = High temperature \n",
    "- yp = Peak demand \n",
    "- xa = Average temperature \n",
    "- ya = Average demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we now use the full data (not just summer)\n",
    "xp = df[\"High_temp\"].values\n",
    "yp = df[\"MAX\"].values\n",
    "xa = df[\"Avg_temp\"].values\n",
    "ya = df[\"AVG\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(xp, yp, marker=\"x\")\n",
    "plt.xlabel(\"High Temperature (Â°C)\")\n",
    "plt.ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotted data looks more like we would expect. Although peak demand increases sharply with temperature after a high temperature of 22 degrees or so (precisely the range when air conditioning usually starts), below this the demand actually starts to increase with lower temperature â though not with as steep a slope, due to the fact that, for example, most heating in Pittsburgh is done with gas rather than with electricity, and other loads that increase with lower temperatures tend to be smaller in magnitude than air conditioning units."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "If we were to fit a linear model to this data, it would look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize, fit and predict\n",
    "lr = LinearRegression()\n",
    "lr.fit(xp.reshape(-1,1), yp)\n",
    "model_pred_p = lr.predict(xp.reshape(-1,1))\n",
    "\n",
    "# plot figure\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(xp, yp, marker=\"x\" )\n",
    "plt.xlabel(\"High Temperature (Â°C)\")\n",
    "plt.ylabel(\"Peak Demand (GW)\")\n",
    "plt.plot(xp, model_pred_p, c='C2')\n",
    "print ('The R^2 of linear regression is: ', r2_score(y_true = yp, y_pred = model_pred_p))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression (quadratic)\n",
    "To capture data of this type, we clearly want some way of expressing nonlinear relationships in the data.  Fortunately, this is possible without actually leaving the domain of so-called \"linear regression\".  The trick we are going to use is a simple one: rather than have features $x^{(i)}$ which only include the \"raw\" inputs such as temperature (plus other raw inputs such as weekday indicators that we saw previously), we are going to build features that include _nonlinear_ functions of the underlying inputs. For example, we could choose the following features\n",
    "\\begin{equation}\n",
    "x^{(i)} = \\left [ \\begin{array}{c} (\\mathrm{HighTemperature}^{(i)})^2 \\\\\n",
    "\\mathrm{HighTemperature}^{(i)} \\\\ 1 \\end{array} \\right ]\n",
    "\\end{equation}\n",
    "which also include a quadratic function of the high temperature variable. If we choose this representation, then our linear hypothesis function $h_\\theta(x) = \\theta^Tx$ is now given by\n",
    "\\begin{equation}\n",
    "h_\\theta(x) = \\theta_1 (\\mathrm{HighTemperature}^{(i)})^2 + \\theta_2 \\mathrm{HighTemperature}^{(i)} + \\theta_3\n",
    "\\end{equation}\n",
    "which is a _quadratic_ function of the high temperature.  Importantly, however, the hypothesis function is still a linear function of the parameters, and so the exact same solution methods work as before (including the exact solution), just by passing a different feature matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first do this by hand. We define a simple function entitled `plot_regression_poly` that takes features, target and the degree of the polynomial as input.\n",
    "\n",
    "In this setting we're actually going to standardize features to the range $[-1,+1]$ even with the least-squares analytical solution for numerical reasons.  High polynomials get very large very quickly, and if we aren't careful it's easy to overload the range of double precision floating point values.\n",
    "\n",
    "To standardize to the range $[-1,+1]$ we use the following formula: \n",
    "\\begin{equation}\n",
    "2*\\frac{x - min(x)}{max(x) - min(x)}-1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is the input variable\n",
    "# y is the output variable\n",
    "# d is degree of polynomial regression\n",
    "\n",
    "def plot_regression_poly(x, y, d):\n",
    "    \n",
    "    # create polynomial features\n",
    "    min_x, max_x = x.min(), x.max()\n",
    "    xs = 2*(x - min_x)/(max_x - min_x)-1  # standardize to range [-1,1]\n",
    "    X = np.array([xs**i for i in range(d,-1,-1)]).T # start at highest polynomial degree (d) and go backwards until 0 in steps of 1\n",
    "    \n",
    "    # implement polynomial regression using least squares (we use the normal equations as derived in the lecture)\n",
    "    theta = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "    \n",
    "    # create scaled test dataset\n",
    "    xt0 = np.linspace(min_x-1, max_x+1, 400) # generate equally spaced x values in the x range\n",
    "    xt = 2 * (xt0 - min_x)/(max_x - min_x) -1 # standardize to range [-1,1]\n",
    "    Xt = np.array([xt**i for i in range(d,-1,-1)]).T # generate polynomial features\n",
    "    yt = Xt @ theta # generate predicted y values\n",
    "    \n",
    "    # plot results\n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.scatter(x, y, marker=\"x\")\n",
    "    ylim = plt.ylim()\n",
    "    plt.plot(xt0, yt, 'C1')\n",
    "    plt.xlabel(\"Temperature (Â°C)\")\n",
    "    plt.ylabel(\"Demand (GW)\")\n",
    "    plt.xlim([min_x-2, max_x+2])\n",
    "    plt.ylim(ylim)\n",
    "    print(theta[:7]) # print first 7 thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_poly(x = xp, y=yp, d = 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better, but a quadratic function is symmetric around its minimum point, and the data we're trying to fit is definitely not symmetric. Thus, we may want a cubic function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression (cubic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of Peak demand vs. High temperature with cubic polynomial regression\n",
    "plot_regression_poly(x = xp, y=yp, d = 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look bad. Let's keep going:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try some other values for d --> What seems to be a good one?\n",
    "plot_regression_poly(x = xp, y=yp, d = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a 100-degree polynomial? Is this something we want? Also, what do you think about the value of the coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_poly(x = xp, y=yp, d = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, something very bad has happened at this point. Especially at the tail ends of the data, where the data points are spaced less closely, we start to get very odd patterns for the data. But the important point is that this is actually a very good fit to the data from a least-squares perspective. As you can see from the figure, the line passes exactly through many of the data point (most obvious on the left hand side of the plot), whereas for the \"better\" fits we had above, our function didn't pass exactly through those points, so actually suffered more loss. But there is an obvious way in which the degree 100 polynomial fit, despite having lower loss, is actually a worse approximation to the underlying data. This brings us to the second topic of these notes, which is the issue of generalization and overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see shortly how we manage these situations, but let us first implement the above example using `scikit learn`. Essentially, all we need to do is to pre-process our feature vector and create polynomial features. To do so, we use a built-in module called `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first consider a toy example to understand how this module works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([[2,3,4], [5,6,7]]).T # let's assume we have two features\n",
    "\n",
    "# initialize PolynomialFeatures\n",
    "PF = PolynomialFeatures(degree = 2,interaction_only = False, include_bias = True)\n",
    "Q_Poly = PF.fit_transform(Q)\n",
    "print(Q)\n",
    "print(Q_Poly)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This will generate a new feature matrix consisting of all polynomial combinations\n",
    "of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]. This is slightly different from what we have done manually above but is a more common way of creating polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_poly_scikit(X,Y,d):\n",
    "    \n",
    "    # initialize PolynomialFeatures\n",
    "    poly_reg = PolynomialFeatures(degree = d)\n",
    "    \n",
    "    # polynomial transformation\n",
    "    x_poly = poly_reg.fit_transform(X.reshape(-1,1))\n",
    "    \n",
    "    # fitting linear regression to polynomial features\n",
    "    lin_reg_Poly = LinearRegression()\n",
    "    lin_reg_Poly.fit(x_poly, Y)\n",
    "    model_pred = lin_reg_Poly.predict(x_poly)\n",
    "    \n",
    "    # plotting the regression line and the data (we have to transform the inputs as well!)\n",
    "    x_fit = np.arange(X.min(), X.max(), 1)\n",
    "    x_fit_poly = poly_reg.fit_transform(x_fit.reshape(-1,1))\n",
    "    y_pred = lin_reg_Poly.predict(x_fit_poly)\n",
    "    \n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.scatter(X,Y,marker=\"x\", c='C2')\n",
    "    ylim = plt.ylim()\n",
    "    plt.plot(x_fit,y_pred, c='C1')\n",
    "    plt.xlabel(\"Temperature (Â°C)\")\n",
    "    plt.ylabel(\"Demand (GW)\")\n",
    "    plt.xlim([X.min()-2,X.max()+2]) # leave some space before and after limits\n",
    "    plt.ylim(ylim)\n",
    "    print ('The R^2 for quadratic curve is: ',r2_score(Y, model_pred))\n",
    "    #print(lin_reg_Poly.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_poly_scikit(X = xp, Y = yp, d = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Overfitting & cross-validation\n",
    "\n",
    "Recall from our previous discussions that the canonical machine learning problem that we solve with every algorithm is the following\n",
    "\\begin{equation}\n",
    "\\underset{\\theta}{minimize} \\; \\frac{1}{m}\\sum_{i=1}^m \\ell \\left(h_\\theta(x^{(i)}),y^{(i)} \\right)\n",
    "\\end{equation}\n",
    "i.e., to minimize the sum of losses on the data set. However, in a more fundamental way, this is not really our goal.  We ultimately do not care about achieving low loss specifically on the points that we are feeding into the algorithm: we already _know_ the true output for each of these data points, and if we want to \"predict\" these points precisely, we could just look up each point in a database (assuming we have some way of referring to the points uniquely).  What we _really_ want from a machine learning algorithm is the ability to predict _new_ data points _of the same type_ as those we trained our model on.  We don't care about knowing what the peak demand _was_ on previous days we have already seen; we care about being able to predict what the peak demand _will be_ tomorrow given the high temperature as input.\n",
    "\n",
    "**Generalization error** \n",
    "\n",
    "This discussion leads us to the notion of _generalization error_.  Informally, the generalization error is just the error (or more generally, the loss) we would experience not on the training data itself, but on new data drawn from the \"same distribution\" as the training data. _This_ is really the quantity we want to minimize, not the error on the training data itself. Because when we run our machine learning algorithm on new data, all that will matter is its generalization performance.\n",
    "\n",
    "What the above example illustrated was an instance of _overfitting_, the situation where the training error is low, but the generalization error is high.  This occurs because we are explicitly trying to minimize the loss on the training set, and if the expressiveness of a hypothesis function is small enough, then we can make this loss arbitrarily small, usually by fitting to pecularities in the training data that provide no real benefit when looking at new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting and model complexity**\n",
    "\n",
    "In general, we expect something like the following behavior when it comes to overfitting.  We're showing the \"cartoon\" form here but we will shortly see what this looks like in practice.\n",
    "\n",
    "![Cartoon illustration of overfitting](overfitting.svg)\n",
    "\n",
    "When model complexity is low, both training and generalization loss are high (here, model complexity can mean any type of representational power of the model, but since we have covered this so far, you can think of it just as the degree of the polynomial in our feature vector).  As we increase model complexity (i.e. add polynomial degrees), then both training and generalization loss start to decrease with training loss usually slightly lower than generalization loss (due to the simple fact that we explicitly optimize training loss).  As we futher increase model complexity, training loss will continue to only decrease: by adding additional representational power to our model, we will only fit the data better and better, since we are explicitly choosing parameters to minimize this loss.  But at a certain point, generalization loss will start to increase again. Our goal when choosing the \"right\" model for a particular machine learning problem is to find the model with lowest generalization error, the minimum of the red line above. However, we cannot do so using the training set alone, because performance on the training set (the blue line) gives us no clue about the generalization performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard cross validation\n",
    "\n",
    "The idea of cross validation is simple.  Given the entirety of our training data, we take some fixed percentage, say 70% of the data, and call this our \"new\" training set.  We then take the remaining data (30% in this case), and call it the _holdout_ or _validation_ set.  The basic idea is that we will use the emprical error or loss on this holdout set as an approximation for the generalization error.  This works because, unlike the training set, we do _not_ choose the parameters of the model based upon the validation set.  This means that there is no way for the parameters to overfit to this data, and thus the validation set provides a reasonable estimate of generalization error even _after_ the parameters have been trained.\n",
    "\n",
    "Let's see how to generate these training and validation sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array\n",
    "X = df[\"High_temp\"]\n",
    "\n",
    "# create randomly shuffled list of indexes \n",
    "np.random.seed(10) # this method is called when RandomState is initialized\n",
    "perm = np.random.permutation(len(X))\n",
    "\n",
    "# select first 70% indices of shuffled list as train set\n",
    "idx_train = perm[:int(len(perm)*0.7)]\n",
    "\n",
    "# select last 30% indices of shuffled list as holdout set\n",
    "idx_cv = perm[int(len(perm)*0.7):]\n",
    "\n",
    "x_train, y_train = df[\"High_temp\"].iloc[idx_train].values, df[\"MAX\"].iloc[idx_train].values\n",
    "x_cv, y_cv = df[\"High_temp\"].iloc[idx_cv].values, df[\"MAX\"].iloc[idx_cv].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(x_train, y_train, marker='x', color=\"C0\")\n",
    "plt.scatter(x_cv, y_cv, marker='x', color=\"C2\")\n",
    "plt.xlabel(\"Temperature (Â°C)\")\n",
    "plt.ylabel(\"Demand (GW)\")\n",
    "plt.legend(['Training set', 'Holdout set'])\n",
    "plt.show()\n",
    "#plt.savefig('crossvalidation.png', dpi = 300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split with `scikit learn`\n",
    "\n",
    "We can also make our life easier and use the `train_test_split` in `scikit learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data with 70%-30% split as above\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[\"High_temp\"], df[\"MAX\"], test_size=0.3,random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a very similar random test split (see figure below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(x_train, y_train, marker='x', color=\"C0\")\n",
    "plt.scatter(x_test, y_test, marker='x', color=\"C2\")\n",
    "plt.xlabel(\"Temperature (Â°C)\")\n",
    "plt.ylabel(\"Demand (GW)\")\n",
    "plt.legend(['Training set', 'Holdout set'])\n",
    "plt.show()\n",
    "#plt.savefig('crossvalidation.pdf', dpi = 300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare training and validation loss\n",
    "\n",
    "Let us know generate the schematic training and validation loss plot from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember our polynomial regression function\n",
    "def ls_poly(x, y, d): # ls=least squares\n",
    "    \n",
    "    # create polynomial features\n",
    "    min_x, max_x = x.min(), x.max()\n",
    "    xs = 2*(x - min_x)/(max_x - min_x)-1  # standardize to range [-1,1]\n",
    "    X = np.array([xs**i for i in range(d,-1,-1)]).T\n",
    "    \n",
    "    # implement polynomial regression using least squares (we use the normal equations as derived in the lecture)\n",
    "    theta = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define min and max of training set\n",
    "min_x_train, max_x_train = x_train.min(), x_train.max()\n",
    "\n",
    "# standardize train and test set based on training set\n",
    "x_train = 2*(x_train - min_x_train)/(max_x_train - min_x_train) - 1\n",
    "x_test = 2*(x_test - min_x_train)/(max_x_train - min_x_train) - 1\n",
    "\n",
    "# create function to generate polynomial feature vector\n",
    "def poly_feat(x, degree):\n",
    "    return np.array([x**i for i in range(degree,-1,-1)]).T\n",
    "    \n",
    "# compute training and validation error for different polynomial degrees\n",
    "err_train = []\n",
    "err_cv = []\n",
    "for i in range(50):\n",
    "    theta = ls_poly(x_train, y_train, i)\n",
    "    err_train.append(((poly_feat(x_train,i) @ theta - y_train)**2).mean())\n",
    "    err_cv.append(((poly_feat(x_test,i) @ theta - y_test)**2).mean())\n",
    "\n",
    "# plot errors\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.semilogy(range(50), err_train, range(50), err_cv)\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"Mean squared error\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in on the \"good\" region..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same code as above, just change range to 20\n",
    "\n",
    "# define min and max of training set\n",
    "min_x_train, max_x_train = x_train.min(), x_train.max()\n",
    "\n",
    "# standardize train and test set based on training set\n",
    "x_train = 2*(x_train - min_x_train)/(max_x_train - min_x_train) - 1\n",
    "x_test = 2*(x_test - min_x_train)/(max_x_train - min_x_train) - 1\n",
    "\n",
    "# create function to generate polynomial feature vector\n",
    "def poly_feat(x, degree):\n",
    "    return np.array([x**i for i in range(degree,-1,-1)]).T\n",
    "    \n",
    "# compute training and validation error for different polynomial degrees\n",
    "err_train = []\n",
    "err_cv = []\n",
    "for i in range(20):\n",
    "    theta = ls_poly(x_train, y_train, i)\n",
    "    err_train.append(((poly_feat(x_train,i) @ theta - y_train)**2).mean())\n",
    "    err_cv.append(((poly_feat(x_test,i) @ theta - y_test)**2).mean())\n",
    "\n",
    "# plot errors\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.semilogy(range(20), err_train, range(20), err_cv)\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"Mean squared error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this plot, one can use the so-called \"elbow method\" to select the appropriate polynomial degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation using train/holdout/test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have discussed in last week's lecture, it is advisable to actually do a three-ways split of the dataset to avoid leakage. You would typically do the following:\n",
    "\n",
    "1. Divide data into training\n",
    "set, holdout set, and test\n",
    "set\n",
    "2. Train algorithm on training\n",
    "set (i.e., to learn\n",
    "parameters), use holdout\n",
    "set to select\n",
    "hyperparameters\n",
    "3. (Optional) retrain model\n",
    "on training + holdout\n",
    "4. Evaluate performance on\n",
    "test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, as above, we can do the split manually. Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array\n",
    "X = df[\"High_temp\"]\n",
    "\n",
    "# create randomly shuffled list of indexes \n",
    "np.random.seed(10) # This method is called when RandomState is initialized\n",
    "perm = np.random.permutation(len(X))\n",
    "\n",
    "# select first 50% indexes of shuffled list as train set\n",
    "idx_train = perm[:int(len(perm)*0.5)]\n",
    "\n",
    "# select 20% indexes of shuffled list as holdout set\n",
    "idx_hold= perm[int(len(perm)*0.5):int(len(perm)*0.7)]\n",
    "\n",
    "# select last 30% indexes of shuffled list as test set\n",
    "idx_test= perm[int(len(perm)*0.7):int(len(perm)*1)]\n",
    "\n",
    "x_train, y_train = df[\"High_temp\"].iloc[idx_train].values, df[\"MAX\"].iloc[idx_train].values\n",
    "x_hold, y_hold = df[\"High_temp\"].iloc[idx_hold].values, df[\"MAX\"].iloc[idx_hold].values\n",
    "x_test, y_test = df[\"High_temp\"].iloc[idx_test].values, df[\"MAX\"].iloc[idx_test].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train),len(x_hold),len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can do it in `scikit learn` a bit more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a 70-30 split first\n",
    "X_train, X_test, y_train, y_test = train_test_split(xp, yp, test_size=0.3,random_state=34)\n",
    "\n",
    "# now split X_train to achive 50-20-30 split\n",
    "X_train, X_hold, y_train, y_hold = train_test_split(X_train, y_train, test_size=(0.2/0.7),random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train),len(X_hold),len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again we can plot the errors to compare\n",
    "min_x_train, max_x_train = X_train.min(), X_train.max()\n",
    "x_train = 2*(X_train - min_x_train)/(max_x_train - min_x_train) - 1\n",
    "x_hold = 2*(X_hold - min_x_train)/(max_x_train - min_x_train) - 1\n",
    "x_test = 2*(X_test - min_x_train)/(max_x_train - min_x_train) - 1\n",
    "\n",
    "def poly_feat(x, degree):\n",
    "    return np.array([x**i for i in range(degree,-1,-1)]).T\n",
    "    \n",
    "err_train = []\n",
    "err_hold = []\n",
    "err_test = []\n",
    "for i in range(30):\n",
    "    theta = ls_poly(x_train, y_train, i)\n",
    "    err_train.append(((poly_feat(x_train,i) @ theta - y_train)**2).mean())\n",
    "    err_hold.append(((poly_feat(x_hold,i) @ theta - y_hold)**2).mean())\n",
    "    err_test.append(((poly_feat(x_test,i) @ theta - y_test)**2).mean())\n",
    "plt.semilogy(range(30), err_train, range(30), err_hold,range(30), err_test)\n",
    "plt.legend([\"Training\", \"Validation\", \"Test\"])\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"Mean squared error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further validation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a whole range of more or less evolved validation techniques. For most of these, pre-built implementations in scikit learn exist. We refer you to the excellent scikit learn cross validation page for further reading. You can access it [here](https://scikit-learn.org/stable/modules/cross_validation.html). The most important technique to check out is k-fold cross-validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

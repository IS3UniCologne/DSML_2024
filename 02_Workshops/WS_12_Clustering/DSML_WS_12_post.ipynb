{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DSML_WS_12` - Introduction to Unsupervised Learning\n",
    "\n",
    "This workshop deals with `unsupervised learning`. In unsupervised learning, we are not trying to predict a continuous value (regression) or a discrete value/class (classification) based on a set of features, but instead we are trying to find (hidden) structures in the data. Therefore, there is no exact measure to tell if your algorithm works well or does not (because we have no fixed reference (such as a label) to compare our results to). During this workshop, we will use clustering algorithms on datasets for which we __do__ have labels, just to give you a better intuition. In real world examples, however, assessing the quality of your clustering result is __more difficult__!\n",
    "\n",
    "We will focus on hard clustering and review the following algorithms:\n",
    "1. **Task: Using tree-based methods to predict electricity demand**\n",
    "1. **Preparation for clustering**\n",
    "1. **K-means clustering**\n",
    "1. **Hierarchichal clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task: Using tree-based methods to predict electricity demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last week's workshop, we talked about decision trees and ensemble methods and applied them to classify breast cancer cells. Let us see how this works in a regression setting, using our electricity demand dataset.\n",
    "\n",
    "- Load data, define your X feature vector to include `Avg_temp` and `is_weekday` (which you first have to generate) and your Y vector to be `AVG`.\n",
    "- Perform a train/test split.\n",
    "- Train four different trees using `DecisionTreeRegressor`, with tree_depths ranging from 2 to 5.\n",
    "- Train a gradient boosting model using `XGBRegressor` (you might have to read up on hyperparameters in the documentation).\n",
    "- Train a random forest model using `RandomForestRegressor` (again, you might need to consult the documentation).\n",
    "- Compare model performance using appropriate test metrics. Which model performs best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"Pittsburgh_load_data.csv\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d.%m.%Y\")\n",
    "df[\"is_weekday\"] = df[\"Date\"].apply(lambda x: 1 if x.weekday() in [0,1,2,3,4] else 0)\n",
    "\n",
    "# define vectors\n",
    "X = df[[\"Avg_temp\", \"is_weekday\"]].values\n",
    "Y = df[\"AVG\"].values\n",
    "\n",
    "# perform train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# train decision trees\n",
    "m1 = DecisionTreeRegressor(max_depth=2).fit(X_train, Y_train)\n",
    "m2 = DecisionTreeRegressor(max_depth=3).fit(X_train, Y_train)\n",
    "m3 = DecisionTreeRegressor(max_depth=4).fit(X_train, Y_train)\n",
    "m4 = DecisionTreeRegressor(max_depth=5).fit(X_train, Y_train)\n",
    "\n",
    "# train xgboost\n",
    "m5 = XGBRegressor(booster=\"gbtree\").fit(X_train, Y_train)\n",
    "\n",
    "# train random forest\n",
    "m6 = RandomForestRegressor(n_estimators=100, \n",
    "                            bootstrap=True,\n",
    "                            random_state=42).fit(X_train, Y_train)\n",
    "\n",
    "# evaluate performance\n",
    "for i, model in enumerate([m1, m2, m3, m4, m5, m6]):\n",
    "    mse = mean_squared_error(Y_test, model.predict(X_test))\n",
    "    r2 = r2_score(Y_test, model.predict(X_test))\n",
    "    print(\"Performance of model \" + str(i+1) + \": MSE = \" + str(mse) + \", R2 Score = \" + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation for clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset used throughout the workshop: Iris flowers\n",
    "For a first intuition, we will be working on the iris dataset again, which you all are familiar with by now. As you remember, in reality there are __three__ different species of irises in the dataset. So, let's see whether we are able to confirm this number of __clusters__ in our dataset using unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('iris.csv', index_col=\"number\").dropna(axis=0)\n",
    "iris.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this an unsupervised learning task we need to drop the label (i.e. `Species`). For later checks we save the response as variable `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop(\"Species\", axis=1)\n",
    "y = iris[\"Species\"]\n",
    "X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep and Scaling\n",
    "First, let's start out by scaling the data. The K-Means algorithm (and any other clustering algorithm for that matter) minimizes some intra-cluster distance (while maximizing inter-cluster distances). For the case of k-means this is typically defined as the **euclidean distance** from the midpoint of each cluster to all points in this cluster. Other distance metrics are also sometimes used (e.g., **Manhattan distance**)\n",
    "\n",
    "If one feature has a bigger spread than others it will be more important than other factors for the outcome of the clustering. We will revisit this later! For now, let's standardize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# create a df out of the array\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "X_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_scaled = X_scaled_df\n",
    "iris_scaled[\"Species\"] = iris[\"Species\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical patterns remain in the scaled data, just at a different scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=iris_scaled, hue=\"Species\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning case (classification)\n",
    "\n",
    "Before we start with the actual clustering, let's revisit the classification methods we talked about during past workshops and see how well they are doing in terms of classification error. For that, we are importing a `kNN classifier` from the sklearn library and have a look at the confusion matrix and the classification report.\n",
    "\n",
    "kNN has some intuitive parallels to k-means clustering (see lecture material)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we are not doing a train-test-split or other advanced methods here; this is not a valid approach for a real supervised learning projects!\n",
    "model_neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "model_neigh.fit(X_scaled,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_neigh.predict(X_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review the performance of our classifier we check relevant test metrics, which you are familiar with by now. We use the `classification_report` class to return a neat summary of the most important classification metrics. We also review the previously discussed `confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"SpeciesPred\"] = y_pred\n",
    "print(classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_conf_matrix = confusion_matrix(y,y_pred)\n",
    "classification_conf_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-means clustering\n",
    "Now, let's drop the labels and just have a look at the given features. How would we group the datapoints for the different observed flowers?\n",
    "In a typical unsupervised learning case we normally just have a rough idea of how many clusters to expect.\n",
    "\n",
    "Three approaches are commonly applied:\n",
    "1. Use expert knowledge\n",
    "1. Plot residual loss for different numbers of clusters, find 'elbow' and select corresponding number of clusters\n",
    "1. Use hierarchical clustering to detect suitable branching and corresponding number of clusters\n",
    "\n",
    "We will focus on the residual loss (elbow method) as a selection criterion for now. In practice, a combination of all three methods is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = 20  # we have 147 datapoints, more than 20 clusters are definitely not reasonable!\n",
    "\n",
    "clusters = []\n",
    "losses = []\n",
    "\n",
    "for k in range(k_max):\n",
    "    model = KMeans(n_clusters=k+1) # initialize\n",
    "    model.fit(X_scaled) # fit\n",
    "    clusters.append(k+1)\n",
    "    losses.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters, losses)\n",
    "plt.xticks(range(k_max+1))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in in the good region by limiting the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clusters, losses)\n",
    "plt.xlim([0,10])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we would expect a good amount of clusters to lie in the region of two to five. Of course, we know that the correct answer is 3 (i.e., one per each species). However, in a true unsupervised learning setting there is no frame of reference and we'd need to draw on several indicators (quantitative and qualitative) to settle on a final number of clusters. For illustrative purposes let's select k=2, the minimum sensible choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-fit algorithm\n",
    "two_means = KMeans(n_clusters=2)\n",
    "two_means.fit(X_scaled)\n",
    "\n",
    "# match records to clusters by calling predict\n",
    "two_means.predict(X_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\"zero\", \"one\", \"two\", \"three\",\"four\",\"five\"]\n",
    "iris_scaled[\"clusters\"] = two_means.predict(X_scaled)\n",
    "iris_scaled[\"clusters\"] = iris_scaled[\"clusters\"].apply(lambda x: numbers[x])\n",
    "sns.pairplot(data=iris_scaled, hue=\"clusters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do this for the \"correct\" number of cluster, i.e., three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_means = KMeans(n_clusters=3)\n",
    "three_means.fit(X_scaled)\n",
    "iris_scaled[\"clusters\"] = three_means.predict(X_scaled)\n",
    "iris_scaled[\"clusters\"] = iris_scaled[\"clusters\"].apply(lambda x: numbers[x])\n",
    "sns.pairplot(data=iris_scaled, hue=\"clusters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would k=5 fare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_means = KMeans(n_clusters=5)\n",
    "five_means.fit(X_scaled)\n",
    "iris_scaled[\"clusters\"] = five_means.predict(X_scaled)\n",
    "iris_scaled[\"clusters\"] = iris_scaled[\"clusters\"].apply(lambda x: numbers[x])\n",
    "sns.pairplot(data=iris_scaled, hue=\"clusters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From inspection, we can tell that k=5 achieves relatively poorer inter-cluster separation (e.g., 1 and 3 are very similar). From these analyses, a data scientist would most likely select 3 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement a second very neat way of evaluating clusters - a silhouette score analysis. The silhouette score is a measure of inter- and intra-cluster distance and can range from -1 (indicating incorrect cluster assignment) to 1 (indicating perfect cluster assignment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # 1st subplot is the silhouette plot\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # initialize k-means with n_clusters value\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(X_scaled)\n",
    "\n",
    "    # silhouette_score gives the average value for all the samples.\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # compute silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_scaled, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # aggregate silhouette scores for samples belonging to cluster i and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # label silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # compute new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"Silhouette plot for various clusters\")\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    # 2nd plot showing actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_scaled[:, 0], X_scaled[:, 1], marker=\".\", s=200, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # labeling clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"Visualization of clustered data\")\n",
    "    ax2.set_xlabel(\"Sepal length\")\n",
    "    ax2.set_ylabel(\"Sepal width\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on iris dataset with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small excursion: Principal Component Analysis (PCA) revisited\n",
    "In the case of the iris dataset, we have just four features - an amount we can still easily manage. But imagine we have a much larger feature space. Then, it might be necessary to reduce the dimensionality of our dataset. Also, we gain another advantage: We can visualize the data more easily! As you have seen above, already with just 4 dimensions it is hard to visualize our data in a meaningful way. The size of a pairplot increases quadratically with the number of features. For four features we have sixteen plots (more than you should expect any stakeholder to have a look at/understand...), but for 10 features it is already a hundred - way more than you would like to have to look at!\n",
    "\n",
    "So, how do we reduce the dimensionality of our dataset without losing too much information? (unfortunately, we are always losing some)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Principal component analysis (PCA) is the way to go! Using PCA, we are looking at our data and try to find a projection which keeps most of the variance within the data. As this is a little theoretical, let's have a quick look at a graphical example:\n",
    "\n",
    "![pca](pca.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our data originally had two dimensions and is reduced onto just one dimension. As you can see, most of the variance of our data is conserved. If the datapoints are very crooked in space, it is a little harder to find the \"principal components\". For this, there a advanced options like, for example, Kernel-PCA (which we are not talking about in this course). \n",
    "\n",
    "Furthermore, there is one more important application for PCA: When there are more features than records in your dataset, you can reduce the dimensionality of your dataset to the number of records without losing any information! So if you have 1 million features but only 100 records, then it is sufficient to look at only the first 100 principal components and you will not have lost any variance. Still, it can be useful to reduce the number of dimensions further in case you do not lose too much variance.\n",
    "\n",
    "As a rule of thumb, there should be at least 95% of the original variance left in the dataset. Let's now run a PCA on our iris dataset and have a look at the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components=2)\n",
    "X_pca2 = pca2.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca2.explained_variance_ratio_)\n",
    "\n",
    "#sum explained variance over all PCA components\n",
    "print(sum(pca2.explained_variance_ratio_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only two principal components, we still have approx. 96% of our variance, so we can assume that the transformed dataset is a good approximation of our original dataset. Let's now have a closer look at how each of these two principal components is composed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca2.components_)\n",
    "print(list(X_scaled_df.columns)[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pca = pd.DataFrame(X_pca2, columns=[\"First PC\", \"Second PC\"], index=iris.index)\n",
    "iris_pca[\"Species\"] = iris[\"Species\"]\n",
    "iris_pca.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running a clustering algorithm on the reduced data, let's first have a look at what we would have seen using only one principal component!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"Species\", y=\"First PC\", data=iris_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are (as expected) already able to differentiate between *setosa* and the other two types easily using just the first principal component. There is a clear difference between the core elements of the other two clusters, but it is not possible to completely differentiate between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"First PC\", y=\"Second PC\", data=iris_pca, fit_reg=False, hue=\"Species\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use K-means again to find clusters in this reduced 2-dimensional space! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_clusters = []\n",
    "pca_losses = []\n",
    "\n",
    "for i in range(k_max):\n",
    "    model = KMeans(n_clusters=i+1)\n",
    "    model.fit(X_pca2)\n",
    "    pca_clusters.append(i+1)\n",
    "    pca_losses.append(model.inertia_)\n",
    "    \n",
    "plt.plot(pca_clusters, pca_losses)\n",
    "plt.xticks(range(k_max+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom into the good region\n",
    "plt.plot(pca_clusters, pca_losses)\n",
    "plt.xlim([0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_three_means = KMeans(n_clusters=3)\n",
    "pca_three_means.fit(X_pca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pca[\"Cluster\"] = pca_three_means.predict(X_pca2)\n",
    "\n",
    "# clustering result\n",
    "sns.lmplot(x=\"First PC\", y=\"Second PC\", hue=\"Cluster\", data=iris_pca, fit_reg=False)\n",
    "# actual classes\n",
    "sns.lmplot(x=\"First PC\", y=\"Second PC\", hue=\"Species\", data=iris_pca, fit_reg=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Clustering\n",
    "\n",
    "Unlike K-Means, hierarchical clustering defines step-wise decision rules for how to seperate the data into clusters. This has the advantage that it is more resembling the human decision making process and is, therefore, very intuitive for stakeholders to understand. \n",
    "\n",
    "\n",
    "Let's have a look at how a supervised decision tree would look like for our iris dataset and then back this up with an unsupervised hierarchical clustering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning Case - Decision Trees for Classification\n",
    "\n",
    "The supervised equivalent of hierarchichal clustering is decision tree classification, which we will briefly re-visit in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "model_tree = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# we engineer a new feature here to enhance performance\n",
    "X[\"Petal.Area\"] = X[\"Petal.Length\"]*X[\"Petal.Width\"]\n",
    "model_tree.fit(X, y)\n",
    "y_pred_tree = model_tree.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y, y_pred=y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(model_tree, class_names=y.unique().tolist(), feature_names=X.columns.tolist(), filled=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning Case - Agglomerative/Hierarchichal Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In agglomerative hierarchical clustering, we begin with every observation in an own cluster and keep joining records into clusters until only one single cluster is left. Obviously, neither extreme is useful, so we need to find the sweet spot in between. Compared to k-means, we do not need to specify the number of clusters in advance. Alternatively, we generate a plot called dendrogram to help us decide on an appropriate number of clusters. Let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "agglo = AgglomerativeClustering(compute_distances=True, n_clusters=3)\n",
    "y_pred_agglo = agglo.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(agglo, labels=agglo.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_hierarchical = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "iris_hierarchical[\"Agglo_clusters\"] = y_pred_agglo\n",
    "iris_hierarchical[\"Species\"] = iris[\"Species\"].values\n",
    "\n",
    "iris_hierarchical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"Sepal.Length\", y=\"Petal.Area\", data=iris_hierarchical, fit_reg=False, hue=\"Agglo_clusters\")\n",
    "sns.lmplot(x=\"Sepal.Length\", y=\"Petal.Area\", hue=\"Species\", data=iris_hierarchical, fit_reg=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
